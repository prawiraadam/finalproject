2022-01-28 18:58:43,195 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,196 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'id'
      (embedding): Embedding(300686, 300)
    )
    (list_embedding_1): WordEmbeddings(
      'id-crawl'
      (embedding): Embedding(1000000, 300)
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=600, out_features=600, bias=True)
  (rnn): LSTM(600, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=20, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2022-01-28 18:58:43,197 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,198 Corpus: "Corpus: 4482 train + 559 dev + 557 test sentences"
2022-01-28 18:58:43,199 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,200 Parameters:
2022-01-28 18:58:43,201  - learning_rate: "0.1"
2022-01-28 18:58:43,202  - mini_batch_size: "32"
2022-01-28 18:58:43,204  - patience: "3"
2022-01-28 18:58:43,205  - anneal_factor: "0.5"
2022-01-28 18:58:43,205  - max_epochs: "10"
2022-01-28 18:58:43,206  - shuffle: "True"
2022-01-28 18:58:43,207  - train_with_dev: "False"
2022-01-28 18:58:43,208  - batch_growth_annealing: "False"
2022-01-28 18:58:43,210 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,211 Model training base path: "resources\taggers\example-upos"
2022-01-28 18:58:43,212 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,215 Device: cuda:0
2022-01-28 18:58:43,218 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:43,219 Embeddings storage mode: cpu
2022-01-28 18:58:43,268 ----------------------------------------------------------------------------------------------------
2022-01-28 18:58:47,673 epoch 1 - iter 14/141 - loss 2.73815282 - samples/sec: 101.74 - lr: 0.100000
2022-01-28 18:58:52,107 epoch 1 - iter 28/141 - loss 2.42652344 - samples/sec: 101.06 - lr: 0.100000
2022-01-28 18:58:56,108 epoch 1 - iter 42/141 - loss 2.19704371 - samples/sec: 112.05 - lr: 0.100000
2022-01-28 18:59:00,220 epoch 1 - iter 56/141 - loss 2.01685630 - samples/sec: 108.99 - lr: 0.100000
2022-01-28 18:59:04,601 epoch 1 - iter 70/141 - loss 1.87474423 - samples/sec: 102.37 - lr: 0.100000
2022-01-28 18:59:09,002 epoch 1 - iter 84/141 - loss 1.76517457 - samples/sec: 101.84 - lr: 0.100000
2022-01-28 18:59:13,574 epoch 1 - iter 98/141 - loss 1.66456958 - samples/sec: 98.01 - lr: 0.100000
2022-01-28 18:59:17,887 epoch 1 - iter 112/141 - loss 1.58436254 - samples/sec: 103.88 - lr: 0.100000
2022-01-28 18:59:22,261 epoch 1 - iter 126/141 - loss 1.51030206 - samples/sec: 102.50 - lr: 0.100000
2022-01-28 18:59:26,126 epoch 1 - iter 140/141 - loss 1.44741859 - samples/sec: 115.98 - lr: 0.100000
2022-01-28 18:59:26,162 ----------------------------------------------------------------------------------------------------
2022-01-28 18:59:26,163 EPOCH 1 done: loss 1.4473 - lr 0.1000000
2022-01-28 18:59:36,372 DEV : loss 0.6211581826210022 - f1-score (micro avg)  0.8069
2022-01-28 18:59:36,412 BAD EPOCHS (no improvement): 0
2022-01-28 18:59:36,414 saving best model
2022-01-28 18:59:51,869 ----------------------------------------------------------------------------------------------------
2022-01-28 18:59:55,304 epoch 2 - iter 14/141 - loss 0.80074724 - samples/sec: 130.52 - lr: 0.100000
2022-01-28 18:59:58,549 epoch 2 - iter 28/141 - loss 0.77481838 - samples/sec: 138.09 - lr: 0.100000
2022-01-28 19:00:01,327 epoch 2 - iter 42/141 - loss 0.76787760 - samples/sec: 161.38 - lr: 0.100000
2022-01-28 19:00:04,735 epoch 2 - iter 56/141 - loss 0.74895267 - samples/sec: 131.50 - lr: 0.100000
2022-01-28 19:00:07,721 epoch 2 - iter 70/141 - loss 0.72668359 - samples/sec: 150.13 - lr: 0.100000
2022-01-28 19:00:10,799 epoch 2 - iter 84/141 - loss 0.71761670 - samples/sec: 145.60 - lr: 0.100000
2022-01-28 19:00:14,158 epoch 2 - iter 98/141 - loss 0.70864929 - samples/sec: 133.45 - lr: 0.100000
2022-01-28 19:00:17,350 epoch 2 - iter 112/141 - loss 0.69743658 - samples/sec: 140.42 - lr: 0.100000
2022-01-28 19:00:20,544 epoch 2 - iter 126/141 - loss 0.68588726 - samples/sec: 140.35 - lr: 0.100000
2022-01-28 19:00:23,850 epoch 2 - iter 140/141 - loss 0.67383220 - samples/sec: 135.54 - lr: 0.100000
2022-01-28 19:00:23,934 ----------------------------------------------------------------------------------------------------
2022-01-28 19:00:23,935 EPOCH 2 done: loss 0.6739 - lr 0.1000000
2022-01-28 19:00:31,305 DEV : loss 0.3888773024082184 - f1-score (micro avg)  0.8682
2022-01-28 19:00:31,344 BAD EPOCHS (no improvement): 0
2022-01-28 19:00:31,346 saving best model
2022-01-28 19:00:47,560 ----------------------------------------------------------------------------------------------------
2022-01-28 19:00:50,414 epoch 3 - iter 14/141 - loss 0.56481969 - samples/sec: 157.10 - lr: 0.100000
2022-01-28 19:00:53,872 epoch 3 - iter 28/141 - loss 0.56528600 - samples/sec: 129.60 - lr: 0.100000
2022-01-28 19:00:57,025 epoch 3 - iter 42/141 - loss 0.56362443 - samples/sec: 142.24 - lr: 0.100000
2022-01-28 19:01:00,533 epoch 3 - iter 56/141 - loss 0.55582546 - samples/sec: 127.74 - lr: 0.100000
2022-01-28 19:01:03,896 epoch 3 - iter 70/141 - loss 0.54486070 - samples/sec: 133.33 - lr: 0.100000
2022-01-28 19:01:07,029 epoch 3 - iter 84/141 - loss 0.53659013 - samples/sec: 143.09 - lr: 0.100000
2022-01-28 19:01:10,378 epoch 3 - iter 98/141 - loss 0.53196578 - samples/sec: 133.89 - lr: 0.100000
2022-01-28 19:01:13,498 epoch 3 - iter 112/141 - loss 0.52932027 - samples/sec: 143.74 - lr: 0.100000
2022-01-28 19:01:16,809 epoch 3 - iter 126/141 - loss 0.52963442 - samples/sec: 135.34 - lr: 0.100000
2022-01-28 19:01:20,410 epoch 3 - iter 140/141 - loss 0.52573410 - samples/sec: 124.46 - lr: 0.100000
2022-01-28 19:01:20,460 ----------------------------------------------------------------------------------------------------
2022-01-28 19:01:20,461 EPOCH 3 done: loss 0.5257 - lr 0.1000000
2022-01-28 19:01:27,530 DEV : loss 0.3303244411945343 - f1-score (micro avg)  0.8908
2022-01-28 19:01:27,569 BAD EPOCHS (no improvement): 0
2022-01-28 19:01:27,571 saving best model
2022-01-28 19:01:43,062 ----------------------------------------------------------------------------------------------------
2022-01-28 19:01:46,312 epoch 4 - iter 14/141 - loss 0.48879112 - samples/sec: 137.99 - lr: 0.100000
2022-01-28 19:01:49,712 epoch 4 - iter 28/141 - loss 0.47719701 - samples/sec: 131.77 - lr: 0.100000
2022-01-28 19:01:53,048 epoch 4 - iter 42/141 - loss 0.48620559 - samples/sec: 134.41 - lr: 0.100000
2022-01-28 19:01:56,236 epoch 4 - iter 56/141 - loss 0.48641908 - samples/sec: 140.59 - lr: 0.100000
2022-01-28 19:01:59,371 epoch 4 - iter 70/141 - loss 0.48119068 - samples/sec: 142.97 - lr: 0.100000
2022-01-28 19:02:02,579 epoch 4 - iter 84/141 - loss 0.47887198 - samples/sec: 139.72 - lr: 0.100000
2022-01-28 19:02:05,886 epoch 4 - iter 98/141 - loss 0.47820019 - samples/sec: 135.50 - lr: 0.100000
2022-01-28 19:02:08,940 epoch 4 - iter 112/141 - loss 0.47538068 - samples/sec: 146.74 - lr: 0.100000
2022-01-28 19:02:12,057 epoch 4 - iter 126/141 - loss 0.47306021 - samples/sec: 143.83 - lr: 0.100000
2022-01-28 19:02:15,374 epoch 4 - iter 140/141 - loss 0.47100909 - samples/sec: 135.12 - lr: 0.100000
2022-01-28 19:02:15,439 ----------------------------------------------------------------------------------------------------
2022-01-28 19:02:15,440 EPOCH 4 done: loss 0.4710 - lr 0.1000000
2022-01-28 19:02:23,686 DEV : loss 0.27696895599365234 - f1-score (micro avg)  0.9098
2022-01-28 19:02:23,725 BAD EPOCHS (no improvement): 0
2022-01-28 19:02:23,727 saving best model
2022-01-28 19:02:40,389 ----------------------------------------------------------------------------------------------------
2022-01-28 19:02:44,150 epoch 5 - iter 14/141 - loss 0.46278273 - samples/sec: 119.18 - lr: 0.100000
2022-01-28 19:02:47,918 epoch 5 - iter 28/141 - loss 0.44875208 - samples/sec: 118.96 - lr: 0.100000
2022-01-28 19:02:51,212 epoch 5 - iter 42/141 - loss 0.44873033 - samples/sec: 136.04 - lr: 0.100000
2022-01-28 19:02:54,381 epoch 5 - iter 56/141 - loss 0.45202890 - samples/sec: 141.48 - lr: 0.100000
2022-01-28 19:02:57,732 epoch 5 - iter 70/141 - loss 0.44388545 - samples/sec: 133.75 - lr: 0.100000
2022-01-28 19:03:00,980 epoch 5 - iter 84/141 - loss 0.44296403 - samples/sec: 138.05 - lr: 0.100000
2022-01-28 19:03:04,804 epoch 5 - iter 98/141 - loss 0.44370382 - samples/sec: 117.22 - lr: 0.100000
2022-01-28 19:03:08,053 epoch 5 - iter 112/141 - loss 0.44180595 - samples/sec: 137.92 - lr: 0.100000
2022-01-28 19:03:11,622 epoch 5 - iter 126/141 - loss 0.44161112 - samples/sec: 125.58 - lr: 0.100000
2022-01-28 19:03:15,023 epoch 5 - iter 140/141 - loss 0.44140680 - samples/sec: 131.81 - lr: 0.100000
2022-01-28 19:03:15,095 ----------------------------------------------------------------------------------------------------
2022-01-28 19:03:15,096 EPOCH 5 done: loss 0.4414 - lr 0.1000000
2022-01-28 19:03:22,536 DEV : loss 0.25653693079948425 - f1-score (micro avg)  0.9125
2022-01-28 19:03:22,575 BAD EPOCHS (no improvement): 0
2022-01-28 19:03:22,577 saving best model
2022-01-28 19:03:37,453 ----------------------------------------------------------------------------------------------------
2022-01-28 19:03:40,769 epoch 6 - iter 14/141 - loss 0.40016486 - samples/sec: 135.22 - lr: 0.100000
2022-01-28 19:03:44,265 epoch 6 - iter 28/141 - loss 0.41341566 - samples/sec: 128.25 - lr: 0.100000
2022-01-28 19:03:47,645 epoch 6 - iter 42/141 - loss 0.41863286 - samples/sec: 132.59 - lr: 0.100000
2022-01-28 19:03:51,265 epoch 6 - iter 56/141 - loss 0.42123060 - samples/sec: 123.85 - lr: 0.100000
2022-01-28 19:03:54,977 epoch 6 - iter 70/141 - loss 0.42209476 - samples/sec: 120.75 - lr: 0.100000
2022-01-28 19:03:58,687 epoch 6 - iter 84/141 - loss 0.41935718 - samples/sec: 120.78 - lr: 0.100000
2022-01-28 19:04:01,960 epoch 6 - iter 98/141 - loss 0.42193658 - samples/sec: 136.95 - lr: 0.100000
2022-01-28 19:04:05,687 epoch 6 - iter 112/141 - loss 0.41950653 - samples/sec: 120.23 - lr: 0.100000
2022-01-28 19:04:08,966 epoch 6 - iter 126/141 - loss 0.41980740 - samples/sec: 136.66 - lr: 0.100000
2022-01-28 19:04:12,359 epoch 6 - iter 140/141 - loss 0.41910188 - samples/sec: 132.12 - lr: 0.100000
2022-01-28 19:04:12,430 ----------------------------------------------------------------------------------------------------
2022-01-28 19:04:12,432 EPOCH 6 done: loss 0.4190 - lr 0.1000000
2022-01-28 19:04:19,799 DEV : loss 0.24810823798179626 - f1-score (micro avg)  0.9161
2022-01-28 19:04:19,839 BAD EPOCHS (no improvement): 0
2022-01-28 19:04:19,841 saving best model
2022-01-28 19:04:36,412 ----------------------------------------------------------------------------------------------------
2022-01-28 19:04:39,728 epoch 7 - iter 14/141 - loss 0.42552622 - samples/sec: 135.22 - lr: 0.100000
2022-01-28 19:04:43,086 epoch 7 - iter 28/141 - loss 0.40304664 - samples/sec: 133.53 - lr: 0.100000
2022-01-28 19:04:46,880 epoch 7 - iter 42/141 - loss 0.41104128 - samples/sec: 118.16 - lr: 0.100000
2022-01-28 19:04:50,298 epoch 7 - iter 56/141 - loss 0.41162590 - samples/sec: 131.11 - lr: 0.100000
2022-01-28 19:04:53,713 epoch 7 - iter 70/141 - loss 0.40934749 - samples/sec: 131.27 - lr: 0.100000
2022-01-28 19:04:57,017 epoch 7 - iter 84/141 - loss 0.41228730 - samples/sec: 135.63 - lr: 0.100000
2022-01-28 19:05:00,453 epoch 7 - iter 98/141 - loss 0.41423854 - samples/sec: 130.43 - lr: 0.100000
2022-01-28 19:05:04,008 epoch 7 - iter 112/141 - loss 0.41440342 - samples/sec: 126.07 - lr: 0.100000
2022-01-28 19:05:07,218 epoch 7 - iter 126/141 - loss 0.41275765 - samples/sec: 139.59 - lr: 0.100000
2022-01-28 19:05:10,470 epoch 7 - iter 140/141 - loss 0.41046184 - samples/sec: 137.83 - lr: 0.100000
2022-01-28 19:05:10,533 ----------------------------------------------------------------------------------------------------
2022-01-28 19:05:10,535 EPOCH 7 done: loss 0.4104 - lr 0.1000000
2022-01-28 19:05:18,216 DEV : loss 0.2527296841144562 - f1-score (micro avg)  0.9169
2022-01-28 19:05:18,256 BAD EPOCHS (no improvement): 0
2022-01-28 19:05:18,258 saving best model
2022-01-28 19:05:33,448 ----------------------------------------------------------------------------------------------------
2022-01-28 19:05:36,857 epoch 8 - iter 14/141 - loss 0.38715363 - samples/sec: 131.61 - lr: 0.100000
2022-01-28 19:05:40,025 epoch 8 - iter 28/141 - loss 0.39310492 - samples/sec: 141.61 - lr: 0.100000
2022-01-28 19:05:43,417 epoch 8 - iter 42/141 - loss 0.39890127 - samples/sec: 132.16 - lr: 0.100000
2022-01-28 19:05:46,757 epoch 8 - iter 56/141 - loss 0.39936775 - samples/sec: 134.21 - lr: 0.100000
2022-01-28 19:05:50,054 epoch 8 - iter 70/141 - loss 0.39653575 - samples/sec: 135.96 - lr: 0.100000
2022-01-28 19:05:53,709 epoch 8 - iter 84/141 - loss 0.39539419 - samples/sec: 122.66 - lr: 0.100000
2022-01-28 19:05:57,559 epoch 8 - iter 98/141 - loss 0.39633217 - samples/sec: 116.43 - lr: 0.100000
2022-01-28 19:06:00,862 epoch 8 - iter 112/141 - loss 0.39585985 - samples/sec: 135.75 - lr: 0.100000
2022-01-28 19:06:04,139 epoch 8 - iter 126/141 - loss 0.39470149 - samples/sec: 136.78 - lr: 0.100000
2022-01-28 19:06:07,556 epoch 8 - iter 140/141 - loss 0.39548723 - samples/sec: 131.19 - lr: 0.100000
2022-01-28 19:06:07,642 ----------------------------------------------------------------------------------------------------
2022-01-28 19:06:07,644 EPOCH 8 done: loss 0.3954 - lr 0.1000000
2022-01-28 19:06:15,089 DEV : loss 0.23838350176811218 - f1-score (micro avg)  0.9179
2022-01-28 19:06:15,129 BAD EPOCHS (no improvement): 0
2022-01-28 19:06:15,131 saving best model
2022-01-28 19:06:31,283 ----------------------------------------------------------------------------------------------------
2022-01-28 19:06:35,056 epoch 9 - iter 14/141 - loss 0.40096695 - samples/sec: 118.84 - lr: 0.100000
2022-01-28 19:06:38,396 epoch 9 - iter 28/141 - loss 0.38764918 - samples/sec: 134.17 - lr: 0.100000
2022-01-28 19:06:41,913 epoch 9 - iter 42/141 - loss 0.38913933 - samples/sec: 127.50 - lr: 0.100000
2022-01-28 19:06:45,156 epoch 9 - iter 56/141 - loss 0.38543028 - samples/sec: 138.17 - lr: 0.100000
2022-01-28 19:06:48,235 epoch 9 - iter 70/141 - loss 0.38815418 - samples/sec: 145.56 - lr: 0.100000
2022-01-28 19:06:51,604 epoch 9 - iter 84/141 - loss 0.38771437 - samples/sec: 133.08 - lr: 0.100000
2022-01-28 19:06:55,420 epoch 9 - iter 98/141 - loss 0.38636371 - samples/sec: 117.44 - lr: 0.100000
2022-01-28 19:06:58,742 epoch 9 - iter 112/141 - loss 0.38675000 - samples/sec: 134.97 - lr: 0.100000
2022-01-28 19:07:02,511 epoch 9 - iter 126/141 - loss 0.38447224 - samples/sec: 118.93 - lr: 0.100000
2022-01-28 19:07:05,635 epoch 9 - iter 140/141 - loss 0.38537671 - samples/sec: 143.42 - lr: 0.100000
2022-01-28 19:07:05,759 ----------------------------------------------------------------------------------------------------
2022-01-28 19:07:05,760 EPOCH 9 done: loss 0.3854 - lr 0.1000000
2022-01-28 19:07:14,045 DEV : loss 0.2513364851474762 - f1-score (micro avg)  0.9147
2022-01-28 19:07:14,085 BAD EPOCHS (no improvement): 1
2022-01-28 19:07:14,087 ----------------------------------------------------------------------------------------------------
2022-01-28 19:07:17,288 epoch 10 - iter 14/141 - loss 0.37995592 - samples/sec: 140.07 - lr: 0.100000
2022-01-28 19:07:20,691 epoch 10 - iter 28/141 - loss 0.38702187 - samples/sec: 131.69 - lr: 0.100000
2022-01-28 19:07:24,239 epoch 10 - iter 42/141 - loss 0.38789792 - samples/sec: 126.36 - lr: 0.100000
2022-01-28 19:07:27,753 epoch 10 - iter 56/141 - loss 0.38430631 - samples/sec: 127.61 - lr: 0.100000
2022-01-28 19:07:31,075 epoch 10 - iter 70/141 - loss 0.38453417 - samples/sec: 134.93 - lr: 0.100000
2022-01-28 19:07:34,419 epoch 10 - iter 84/141 - loss 0.37880834 - samples/sec: 134.05 - lr: 0.100000
2022-01-28 19:07:37,785 epoch 10 - iter 98/141 - loss 0.37724967 - samples/sec: 133.18 - lr: 0.100000
2022-01-28 19:07:41,290 epoch 10 - iter 112/141 - loss 0.37819526 - samples/sec: 127.87 - lr: 0.100000
2022-01-28 19:07:44,592 epoch 10 - iter 126/141 - loss 0.37770307 - samples/sec: 135.71 - lr: 0.100000
2022-01-28 19:07:47,997 epoch 10 - iter 140/141 - loss 0.37873632 - samples/sec: 131.81 - lr: 0.100000
2022-01-28 19:07:48,070 ----------------------------------------------------------------------------------------------------
2022-01-28 19:07:48,071 EPOCH 10 done: loss 0.3788 - lr 0.1000000
2022-01-28 19:07:55,445 DEV : loss 0.23448392748832703 - f1-score (micro avg)  0.9206
2022-01-28 19:07:55,487 BAD EPOCHS (no improvement): 0
2022-01-28 19:07:55,489 saving best model
2022-01-28 19:08:27,984 ----------------------------------------------------------------------------------------------------
2022-01-28 19:08:27,986 loading file resources\taggers\example-upos\best-model.pt
2022-01-28 19:08:39,846 0.9251	0.9251	0.9251	0.9251
2022-01-28 19:08:39,847 
Results:
- F-score (micro) 0.9251
- F-score (macro) 0.8646
- Accuracy 0.9251

By class:
              precision    recall  f1-score   support

        NOUN     0.8748    0.9152    0.8945      2511
       PROPN     0.9271    0.8996    0.9131      2162
       PUNCT     0.9982    1.0000    0.9991      1623
        VERB     0.9454    0.9342    0.9398      1261
         ADP     0.9334    0.9560    0.9446      1114
        PRON     0.9321    0.9798    0.9553       644
         ADJ     0.8614    0.7131    0.7803       488
         NUM     0.9352    0.9766    0.9554       384
       CCONJ     0.9783    0.9945    0.9863       362
         ADV     0.8433    0.7775    0.8090       346
         DET     0.9599    0.9120    0.9353       341
         AUX     0.9306    0.9956    0.9620       229
       SCONJ     0.8500    0.7887    0.8182       194
        PART     0.9149    0.9663    0.9399        89
         SYM     1.0000    1.0000    1.0000         6
           X     0.0000    0.0000    0.0000         2

   micro avg     0.9251    0.9251    0.9251     11756
   macro avg     0.8678    0.8631    0.8646     11756
weighted avg     0.9247    0.9251    0.9243     11756
 samples avg     0.9251    0.9251    0.9251     11756

2022-01-28 19:08:39,848 ----------------------------------------------------------------------------------------------------
