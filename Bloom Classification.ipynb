{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df = pd.read_csv('data\\data_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Dataset Distribution')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grafik kiri\n",
    "label = df['label'].value_counts().sort_index(ascending=True)\n",
    "label_pt = label.index\n",
    "label_freq = label.values\n",
    "\n",
    "# Grafik tengah\n",
    "ed_level = df['edu_level'].value_counts()\n",
    "edu_level_pt = ed_level.index\n",
    "edu_level_freq = ed_level.values\n",
    "\n",
    "# Grafik kanan\n",
    "subject = df['subject'].value_counts()\n",
    "subject_pt = subject.index\n",
    "subject_freq = subject.values\n",
    "\n",
    "# Subplot dibuat 1 baris, 3 kolom. figsize=(x, y)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Grafik kiri\n",
    "axs[0].bar(label_pt, label_freq)\n",
    "axs[0].set_xlabel(\"Cognitive Level\")\n",
    "axs[0].set_ylabel(\"Num Of Questions\")\n",
    "\n",
    "# Grafik tengah\n",
    "axs[1].bar(edu_level_pt, edu_level_freq)\n",
    "axs[1].set_xlabel(\"Education Level\")\n",
    "axs[1].set_ylabel(\"Num Of Questions\")\n",
    "\n",
    "# Grafik kanan\n",
    "axs[2].bar(subject_pt, subject_freq)\n",
    "axs[2].set_xlabel(\"Subject\")\n",
    "axs[2].set_ylabel(\"Num Of Questions\")\n",
    "\n",
    "fig.suptitle('Dataset Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-29 17:35:45,321 loading file resources/taggers/example-upos/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import re\n",
    "\n",
    "# Loading trained model hasil dari FlairNLP untuk POS Tagging\n",
    "postagger = SequenceTagger.load('resources/taggers/example-upos/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tag(series):\n",
    "    word_tag = []\n",
    "    for i in range(series.shape[0]):\n",
    "        text = Sentence(series[i])\n",
    "        postagger.predict(text)\n",
    "        \n",
    "        # hasil dari FlairNLP berbentuk [WORD, <TAG>]\n",
    "        # karakter kurung sudut dihilangkan\n",
    "        x = re.split(r\"\\>\\s|>\", text.to_tagged_string())\n",
    "        res = []\n",
    "        \n",
    "        # Untuk split biar dapet pasangan kata-tag\n",
    "        for i in range(len(x)):\n",
    "            temp = re.split(r\"\\s\", x[i])\n",
    "            if(len(temp) > 1):\n",
    "                for char in temp[1]:\n",
    "                    if char == '<':\n",
    "                        temp[1] = temp[1].replace('<', \"\")\n",
    "                res.append(temp)\n",
    "        word_tag.append(res)\n",
    "    \n",
    "    return word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>tag_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udang ronggeng memiliki duri-duri yang keras, ...</td>\n",
       "      <td>Makna istilah kata vulkanis pada kutipan teks ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[Makna, NOUN], [istilah, NOUN], [kata, NOUN],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya pun membandingkan perlakuan yang ibu beri...</td>\n",
       "      <td>Latar suasana pada paragraf pertama dalam kuti...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[Latar, NOUN], [suasana, NOUN], [pada, ADP], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bapakku bernama Narto. Biasa dipanggil Kang Na...</td>\n",
       "      <td>Makna frasa cokelat legam pada kutipan cerpen ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[Makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kebakaran hutan akibat kelalaian manusia di be...</td>\n",
       "      <td>Maksud pernyataan Evakuasi akan dilakukan kepa...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[Maksud, NOUN], [pernyataan, NOUN], [Evakuasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hal yang disarankan Mat agar dilakukan istriny...</td>\n",
       "      <td>Nilai moral pada kutipan novel tersebut adalah ….</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[Nilai, NOUN], [moral, NOUN], [pada, ADP], [k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Udang ronggeng memiliki duri-duri yang keras, ...   \n",
       "1  Saya pun membandingkan perlakuan yang ibu beri...   \n",
       "2  Bapakku bernama Narto. Biasa dipanggil Kang Na...   \n",
       "3  Kebakaran hutan akibat kelalaian manusia di be...   \n",
       "4  Hal yang disarankan Mat agar dilakukan istriny...   \n",
       "\n",
       "                                            question label           subject  \\\n",
       "0  Makna istilah kata vulkanis pada kutipan teks ...    C2  bahasa indonesia   \n",
       "1  Latar suasana pada paragraf pertama dalam kuti...    C2  bahasa indonesia   \n",
       "2  Makna frasa cokelat legam pada kutipan cerpen ...    C2  bahasa indonesia   \n",
       "3  Maksud pernyataan Evakuasi akan dilakukan kepa...    C2  bahasa indonesia   \n",
       "4  Nilai moral pada kutipan novel tersebut adalah ….    C2  bahasa indonesia   \n",
       "\n",
       "  edu_level                                           tag_pair  \n",
       "0       SMA  [[Makna, NOUN], [istilah, NOUN], [kata, NOUN],...  \n",
       "1       SMA  [[Latar, NOUN], [suasana, NOUN], [pada, ADP], ...  \n",
       "2       SMA  [[Makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...  \n",
       "3       SMA  [[Maksud, NOUN], [pernyataan, NOUN], [Evakuasi...  \n",
       "4       SMA  [[Nilai, NOUN], [moral, NOUN], [pada, ADP], [k...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag_pair'] = add_tag(df.question)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>tag_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udang ronggeng memiliki duri-duri yang keras, ...</td>\n",
       "      <td>Makna istilah kata vulkanis pada kutipan teks ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [kata, NOUN],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya pun membandingkan perlakuan yang ibu beri...</td>\n",
       "      <td>Latar suasana pada paragraf pertama dalam kuti...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [pada, ADP], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bapakku bernama Narto. Biasa dipanggil Kang Na...</td>\n",
       "      <td>Makna frasa cokelat legam pada kutipan cerpen ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kebakaran hutan akibat kelalaian manusia di be...</td>\n",
       "      <td>Maksud pernyataan Evakuasi akan dilakukan kepa...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[maksud, NOUN], [pernyataan, NOUN], [evakuasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hal yang disarankan Mat agar dilakukan istriny...</td>\n",
       "      <td>Nilai moral pada kutipan novel tersebut adalah ….</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [pada, ADP], [k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Udang ronggeng memiliki duri-duri yang keras, ...   \n",
       "1  Saya pun membandingkan perlakuan yang ibu beri...   \n",
       "2  Bapakku bernama Narto. Biasa dipanggil Kang Na...   \n",
       "3  Kebakaran hutan akibat kelalaian manusia di be...   \n",
       "4  Hal yang disarankan Mat agar dilakukan istriny...   \n",
       "\n",
       "                                            question label           subject  \\\n",
       "0  Makna istilah kata vulkanis pada kutipan teks ...    C2  bahasa indonesia   \n",
       "1  Latar suasana pada paragraf pertama dalam kuti...    C2  bahasa indonesia   \n",
       "2  Makna frasa cokelat legam pada kutipan cerpen ...    C2  bahasa indonesia   \n",
       "3  Maksud pernyataan Evakuasi akan dilakukan kepa...    C2  bahasa indonesia   \n",
       "4  Nilai moral pada kutipan novel tersebut adalah ….    C2  bahasa indonesia   \n",
       "\n",
       "  edu_level                                           tag_pair  \n",
       "0       SMA  [[makna, NOUN], [istilah, NOUN], [kata, NOUN],...  \n",
       "1       SMA  [[latar, NOUN], [suasana, NOUN], [pada, ADP], ...  \n",
       "2       SMA  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...  \n",
       "3       SMA  [[maksud, NOUN], [pernyataan, NOUN], [evakuasi...  \n",
       "4       SMA  [[nilai, NOUN], [moral, NOUN], [pada, ADP], [k...  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casefolding from column 'question'\n",
    "# df.question = df.question.str.lower()\n",
    "\n",
    "# Mengubah semua huruf kapital menjadi huruf kecil dari kolom 'tag_pair'\n",
    "for i in range(df.tag_pair.shape[0]):\n",
    "    for j in range(len(df['tag_pair'][i])):\n",
    "        df['tag_pair'][i][j][0] = df['tag_pair'][i][j][0].lower()\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(series):\n",
    "    no_punct = []\n",
    "    for i in range(series.shape[0]):\n",
    "        holder = series[i]\n",
    "        temp = []\n",
    "        for j in range(len(holder)):\n",
    "        \n",
    "            # Mapping punctuations dari string ke ''\n",
    "            holder[j][0] = holder[j][0].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            # Hapus tag bertanda 'PUNCT'\n",
    "            # Dibuat list baru untuk nampung non-PUNCT\n",
    "            if len(holder[j][0]) > 0 and holder[j][1] != 'PUNCT':\n",
    "                temp.append(holder[j])\n",
    "        \n",
    "        no_punct.append(temp)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag_pair'] = remove_punctuation(df['tag_pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>tag_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udang ronggeng memiliki duri-duri yang keras, ...</td>\n",
       "      <td>Makna istilah kata vulkanis pada kutipan teks ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [kata, NOUN],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya pun membandingkan perlakuan yang ibu beri...</td>\n",
       "      <td>Latar suasana pada paragraf pertama dalam kuti...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [pada, ADP], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bapakku bernama Narto. Biasa dipanggil Kang Na...</td>\n",
       "      <td>Makna frasa cokelat legam pada kutipan cerpen ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kebakaran hutan akibat kelalaian manusia di be...</td>\n",
       "      <td>Maksud pernyataan Evakuasi akan dilakukan kepa...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[maksud, NOUN], [pernyataan, NOUN], [evakuasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hal yang disarankan Mat agar dilakukan istriny...</td>\n",
       "      <td>Nilai moral pada kutipan novel tersebut adalah ….</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [pada, ADP], [k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Udang ronggeng memiliki duri-duri yang keras, ...   \n",
       "1  Saya pun membandingkan perlakuan yang ibu beri...   \n",
       "2  Bapakku bernama Narto. Biasa dipanggil Kang Na...   \n",
       "3  Kebakaran hutan akibat kelalaian manusia di be...   \n",
       "4  Hal yang disarankan Mat agar dilakukan istriny...   \n",
       "\n",
       "                                            question label           subject  \\\n",
       "0  Makna istilah kata vulkanis pada kutipan teks ...    C2  bahasa indonesia   \n",
       "1  Latar suasana pada paragraf pertama dalam kuti...    C2  bahasa indonesia   \n",
       "2  Makna frasa cokelat legam pada kutipan cerpen ...    C2  bahasa indonesia   \n",
       "3  Maksud pernyataan Evakuasi akan dilakukan kepa...    C2  bahasa indonesia   \n",
       "4  Nilai moral pada kutipan novel tersebut adalah ….    C2  bahasa indonesia   \n",
       "\n",
       "  edu_level                                           tag_pair  \n",
       "0       SMA  [[makna, NOUN], [istilah, NOUN], [kata, NOUN],...  \n",
       "1       SMA  [[latar, NOUN], [suasana, NOUN], [pada, ADP], ...  \n",
       "2       SMA  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...  \n",
       "3       SMA  [[maksud, NOUN], [pernyataan, NOUN], [evakuasi...  \n",
       "4       SMA  [[nilai, NOUN], [moral, NOUN], [pada, ADP], [k...  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bikin copy isi dataframe\n",
    "# Work-around karena value setelah pake df.copy() suka berubah sendiri\n",
    "def reserve(df_col):\n",
    "    res = []\n",
    "    for index in df_col.index:\n",
    "        holder = df_col[index]\n",
    "        temp = []\n",
    "        for i in range(len(holder)):\n",
    "            temp_2 = []\n",
    "            temp_2.append(holder[i][0])\n",
    "            temp_2.append(holder[i][1])\n",
    "            temp.append(temp_2)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambah 0 ke list reserve untuk keperluan weighting\n",
    "def add_zero(list_2d):\n",
    "    for i in range(len(list_2d)):\n",
    "        for j in range(len(list_2d[i])):\n",
    "            list_2d[i][j].append(0)\n",
    "    return list_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    features_with_0 = []\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        temp = []\n",
    "        temp.append(features[i])\n",
    "        temp.append(0)\n",
    "        features_with_0.append(temp)\n",
    "\n",
    "    wordfreq = X.toarray()\n",
    "    for i in range(len(wordfreq)):\n",
    "        for j in range(len(wordfreq[i])):\n",
    "            if wordfreq[i][j] != 0:\n",
    "                features_with_0[j][1] += wordfreq[i][j]\n",
    "    \n",
    "    return features_with_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(series, stopwords):\n",
    "    res = []\n",
    "    for i in range(series.shape[0]):\n",
    "        temp = []\n",
    "        holder = series[i]\n",
    "        for j in range(len(holder)):\n",
    "            if holder[j][0] in stopwords:\n",
    "                holder[j][0] = ''\n",
    "            if len(holder[j][0]) != 0:\n",
    "                temp.append(holder[j])\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword = stopword_factory.create_stop_word_remover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_default = stopword_factory.get_stop_words()\n",
    "sw_keep = ['adalah', 'apa', 'arti', 'artinya', 'berapa', 'berapakah', 'beri', \n",
    "           'berikan', 'diantaranya', 'disebut', 'jelaskan', 'karena',  \n",
    "           'mengapa', 'menunjukkan', 'merupakan', 'rupa', 'sebut']\n",
    "\n",
    "# List Comprehension buat exclude stopwords di sw_keep dari sw\n",
    "sw_modify = [x for x in sw_default if x not in sw_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikat kolom tag_pair\n",
    "reserve_pair = reserve(df['tag_pair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['makna', 'NOUN'], ['istilah', 'NOUN'], ['kata', 'NOUN'], ['vulkanis', 'NOUN'], ['pada', 'ADP'], ['kutipan', 'NOUN'], ['teks', 'NOUN'], ['tersebut', 'DET'], ['adalah', 'AUX']]\n"
     ]
    }
   ],
   "source": [
    "# tag_pair dibikin reserve soalnya habis buang stopword jadi rusak\n",
    "df['tag_pair_def'] = remove_stopword(df['tag_pair'], sw_default)\n",
    "df['tag_pair'] = reserve_pair\n",
    "print(df['tag_pair'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag_pair_mod'] = remove_stopword(df['tag_pair'], sw_modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['makna', 'NOUN'],\n",
       " ['istilah', 'NOUN'],\n",
       " ['vulkanis', 'NOUN'],\n",
       " ['kutipan', 'NOUN'],\n",
       " ['teks', 'NOUN']]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag_pair_def'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['makna', 'NOUN'],\n",
       " ['istilah', 'NOUN'],\n",
       " ['vulkanis', 'NOUN'],\n",
       " ['kutipan', 'NOUN'],\n",
       " ['teks', 'NOUN'],\n",
       " ['adalah', 'AUX']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag_pair_mod'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>tag_pair</th>\n",
       "      <th>tag_pair_def</th>\n",
       "      <th>tag_pair_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udang ronggeng memiliki duri-duri yang keras, ...</td>\n",
       "      <td>Makna istilah kata vulkanis pada kutipan teks ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [, NOUN], [vu...</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [vulkanis, NO...</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [vulkanis, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya pun membandingkan perlakuan yang ibu beri...</td>\n",
       "      <td>Latar suasana pada paragraf pertama dalam kuti...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [, ADP], [par...</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [paragraf, NO...</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [paragraf, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bapakku bernama Narto. Biasa dipanggil Kang Na...</td>\n",
       "      <td>Makna frasa cokelat legam pada kutipan cerpen ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kebakaran hutan akibat kelalaian manusia di be...</td>\n",
       "      <td>Maksud pernyataan Evakuasi akan dilakukan kepa...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[, NOUN], [pernyataan, NOUN], [evakuasi, NOUN...</td>\n",
       "      <td>[[pernyataan, NOUN], [evakuasi, NOUN], [masyar...</td>\n",
       "      <td>[[pernyataan, NOUN], [evakuasi, NOUN], [masyar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hal yang disarankan Mat agar dilakukan istriny...</td>\n",
       "      <td>Nilai moral pada kutipan novel tersebut adalah ….</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [, ADP], [kutip...</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [kutipan, NOUN]...</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [kutipan, NOUN]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Udang ronggeng memiliki duri-duri yang keras, ...   \n",
       "1  Saya pun membandingkan perlakuan yang ibu beri...   \n",
       "2  Bapakku bernama Narto. Biasa dipanggil Kang Na...   \n",
       "3  Kebakaran hutan akibat kelalaian manusia di be...   \n",
       "4  Hal yang disarankan Mat agar dilakukan istriny...   \n",
       "\n",
       "                                            question label           subject  \\\n",
       "0  Makna istilah kata vulkanis pada kutipan teks ...    C2  bahasa indonesia   \n",
       "1  Latar suasana pada paragraf pertama dalam kuti...    C2  bahasa indonesia   \n",
       "2  Makna frasa cokelat legam pada kutipan cerpen ...    C2  bahasa indonesia   \n",
       "3  Maksud pernyataan Evakuasi akan dilakukan kepa...    C2  bahasa indonesia   \n",
       "4  Nilai moral pada kutipan novel tersebut adalah ….    C2  bahasa indonesia   \n",
       "\n",
       "  edu_level                                           tag_pair  \\\n",
       "0       SMA  [[makna, NOUN], [istilah, NOUN], [, NOUN], [vu...   \n",
       "1       SMA  [[latar, NOUN], [suasana, NOUN], [, ADP], [par...   \n",
       "2       SMA  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...   \n",
       "3       SMA  [[, NOUN], [pernyataan, NOUN], [evakuasi, NOUN...   \n",
       "4       SMA  [[nilai, NOUN], [moral, NOUN], [, ADP], [kutip...   \n",
       "\n",
       "                                        tag_pair_def  \\\n",
       "0  [[makna, NOUN], [istilah, NOUN], [vulkanis, NO...   \n",
       "1  [[latar, NOUN], [suasana, NOUN], [paragraf, NO...   \n",
       "2  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...   \n",
       "3  [[pernyataan, NOUN], [evakuasi, NOUN], [masyar...   \n",
       "4  [[nilai, NOUN], [moral, NOUN], [kutipan, NOUN]...   \n",
       "\n",
       "                                        tag_pair_mod  \n",
       "0  [[makna, NOUN], [istilah, NOUN], [vulkanis, NO...  \n",
       "1  [[latar, NOUN], [suasana, NOUN], [paragraf, NO...  \n",
       "2  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...  \n",
       "3  [[pernyataan, NOUN], [evakuasi, NOUN], [masyar...  \n",
       "4  [[nilai, NOUN], [moral, NOUN], [kutipan, NOUN]...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Mengubah kata ke bentuk dasarnya\n",
    "# Stemming setelah POSTagging karena sequence kata berguna dalam POSTagging\n",
    "def stem(series):\n",
    "    for i in range(series.shape[0]):\n",
    "        holder = series[i]\n",
    "        for j in range(len(holder)):\n",
    "            holder[j][0] = stemmer.stem(holder[j][0])\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['makna', 'NOUN'],\n",
       " ['istilah', 'NOUN'],\n",
       " ['vulkanis', 'NOUN'],\n",
       " ['kutip', 'NOUN'],\n",
       " ['teks', 'NOUN']]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tag_pair_def = stem(df.tag_pair_def)\n",
    "df.tag_pair_def[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['makna', 'NOUN'],\n",
       " ['istilah', 'NOUN'],\n",
       " ['vulkanis', 'NOUN'],\n",
       " ['kutip', 'NOUN'],\n",
       " ['teks', 'NOUN'],\n",
       " ['adalah', 'AUX']]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tag_pair_mod = stem(df.tag_pair_mod)\n",
    "df.tag_pair_mod[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>tag_pair</th>\n",
       "      <th>tag_pair_def</th>\n",
       "      <th>tag_pair_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udang ronggeng memiliki duri-duri yang keras, ...</td>\n",
       "      <td>Makna istilah kata vulkanis pada kutipan teks ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [, NOUN], [vu...</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [vulkanis, NO...</td>\n",
       "      <td>[[makna, NOUN], [istilah, NOUN], [vulkanis, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saya pun membandingkan perlakuan yang ibu beri...</td>\n",
       "      <td>Latar suasana pada paragraf pertama dalam kuti...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [, ADP], [par...</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [paragraf, NO...</td>\n",
       "      <td>[[latar, NOUN], [suasana, NOUN], [paragraf, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bapakku bernama Narto. Biasa dipanggil Kang Na...</td>\n",
       "      <td>Makna frasa cokelat legam pada kutipan cerpen ...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "      <td>[[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kebakaran hutan akibat kelalaian manusia di be...</td>\n",
       "      <td>Maksud pernyataan Evakuasi akan dilakukan kepa...</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[, NOUN], [nyata, NOUN], [evakuasi, NOUN], [,...</td>\n",
       "      <td>[[nyata, NOUN], [evakuasi, NOUN], [masyarakat,...</td>\n",
       "      <td>[[nyata, NOUN], [evakuasi, NOUN], [masyarakat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hal yang disarankan Mat agar dilakukan istriny...</td>\n",
       "      <td>Nilai moral pada kutipan novel tersebut adalah ….</td>\n",
       "      <td>C2</td>\n",
       "      <td>bahasa indonesia</td>\n",
       "      <td>SMA</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [, ADP], [kutip...</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [kutip, NOUN], ...</td>\n",
       "      <td>[[nilai, NOUN], [moral, NOUN], [kutip, NOUN], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Udang ronggeng memiliki duri-duri yang keras, ...   \n",
       "1  Saya pun membandingkan perlakuan yang ibu beri...   \n",
       "2  Bapakku bernama Narto. Biasa dipanggil Kang Na...   \n",
       "3  Kebakaran hutan akibat kelalaian manusia di be...   \n",
       "4  Hal yang disarankan Mat agar dilakukan istriny...   \n",
       "\n",
       "                                            question label           subject  \\\n",
       "0  Makna istilah kata vulkanis pada kutipan teks ...    C2  bahasa indonesia   \n",
       "1  Latar suasana pada paragraf pertama dalam kuti...    C2  bahasa indonesia   \n",
       "2  Makna frasa cokelat legam pada kutipan cerpen ...    C2  bahasa indonesia   \n",
       "3  Maksud pernyataan Evakuasi akan dilakukan kepa...    C2  bahasa indonesia   \n",
       "4  Nilai moral pada kutipan novel tersebut adalah ….    C2  bahasa indonesia   \n",
       "\n",
       "  edu_level                                           tag_pair  \\\n",
       "0       SMA  [[makna, NOUN], [istilah, NOUN], [, NOUN], [vu...   \n",
       "1       SMA  [[latar, NOUN], [suasana, NOUN], [, ADP], [par...   \n",
       "2       SMA  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...   \n",
       "3       SMA  [[, NOUN], [nyata, NOUN], [evakuasi, NOUN], [,...   \n",
       "4       SMA  [[nilai, NOUN], [moral, NOUN], [, ADP], [kutip...   \n",
       "\n",
       "                                        tag_pair_def  \\\n",
       "0  [[makna, NOUN], [istilah, NOUN], [vulkanis, NO...   \n",
       "1  [[latar, NOUN], [suasana, NOUN], [paragraf, NO...   \n",
       "2  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...   \n",
       "3  [[nyata, NOUN], [evakuasi, NOUN], [masyarakat,...   \n",
       "4  [[nilai, NOUN], [moral, NOUN], [kutip, NOUN], ...   \n",
       "\n",
       "                                        tag_pair_mod  \n",
       "0  [[makna, NOUN], [istilah, NOUN], [vulkanis, NO...  \n",
       "1  [[latar, NOUN], [suasana, NOUN], [paragraf, NO...  \n",
       "2  [[makna, NOUN], [frasa, NOUN], [cokelat, NOUN]...  \n",
       "3  [[nyata, NOUN], [evakuasi, NOUN], [masyarakat,...  \n",
       "4  [[nilai, NOUN], [moral, NOUN], [kutip, NOUN], ...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserve_pair = add_zero(reserve_pair)\n",
    "reserve_pair_mod = add_zero(reserve_pair_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reserve_pair[0])\n",
    "print(reserve_pair_mod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    holder = row['tag_pair_mod']\n",
    "    for i in range(len(holder)):\n",
    "        if holder[i][1] not in tags:\n",
    "            tags.append(holder[i][1])\n",
    "            \n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_def = []\n",
    "text_mod = []\n",
    "\n",
    "for index in df.index:\n",
    "    holder_def = df['tag_pair'][index]\n",
    "    temp_def = []\n",
    "    holder_mod = df['tag_pair_mod'][index]\n",
    "    temp_mod = []\n",
    "    \n",
    "    for i in range(len(holder_def)):\n",
    "        temp_def.append(holder_def[i][0])\n",
    "    for i in range(len(holder_mod)):\n",
    "        temp_mod.append(holder_mod[i][0])\n",
    "    \n",
    "    text_def.append(' '.join(temp_def))\n",
    "    text_mod.append(' '.join(temp_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq_def = count_freq(text_def)\n",
    "wordfreq_def.sort(key=lambda x: int(x[1]))\n",
    "\n",
    "wordfreq_mod = count_freq(text_mod)\n",
    "wordfreq_mod.sort(key=lambda x: int(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(term_doc_matrix):\n",
    "    # Convert sparse matrix to list\n",
    "    temp = []\n",
    "    for i in range(term_doc_matrix.shape[0]):\n",
    "        temp.append(term_doc_matrix[i].toarray().tolist())\n",
    "    # Flatten list\n",
    "    res = list(chain.from_iterable(temp))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(word, wordfreq):\n",
    "    for i in range(len(wordfreq)):\n",
    "        if wordfreq[i][1] == word:\n",
    "            return word\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfpos_denom(wordfreq, df):\n",
    "    res = 0   \n",
    "    \n",
    "    for i in range(len(wordfreq)):\n",
    "        for j in range(df.shape[0]):\n",
    "            if df[j][0][0] == wordfreq[i][0]:\n",
    "                w = 0\n",
    "                if df[j][0][1] == 'VERB':\n",
    "                    w = 5\n",
    "                elif df[j][0][1] == 'ADJ' or df[j][0][1] == 'NOUN':\n",
    "                    w = 3\n",
    "                else:\n",
    "                    w = 1           \n",
    "                res += wordfreq[i][0]*w\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postag_weighting(tfidf, df, wordfreq):\n",
    "    # to_array dari hasil vectorizer -> set i = 0, cek if value non zero then i++ -> cek ke df['tag_pair']\n",
    "    #               sesuaiin POSTag by i -> apply weighting\n",
    "    result = []\n",
    "    denom = get_tfpos_denom(wordfreq, df)\n",
    "    \n",
    "    for i in range(len(tfidf)):\n",
    "        tfidf_holder = tfidf[i]\n",
    "        pair_holder = df.iloc[i]\n",
    "        weighted = []\n",
    "        idx = 0\n",
    "        \n",
    "        for j in range(len(tfidf_holder)):\n",
    "            weight = 1\n",
    "            if tfidf_holder[j] > 0:\n",
    "                if pair_holder[idx][1] == 'VERB':\n",
    "                    weight = 5\n",
    "                elif pair_holder[idx][1] == 'ADJ' or pair_holder[idx][1] == 'NOUN': \n",
    "                    weight = 3\n",
    "                else:\n",
    "                    weight = 1\n",
    "                    \n",
    "                freq = get_freq(pair_holder[idx][0], wordfreq)\n",
    "                idx += 1\n",
    "                \n",
    "            weighted.append(tfidf_holder[j]*weight)\n",
    "        result.append(weighted)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Buat ngitung nilai TF-IDF dari tiap term\n",
    "# Vocabulary isinya daftar kata yang dipake di corpus\n",
    "term_doc_matrix_def = vectorizer.fit_transform(text_def)\n",
    "# vocabulary_def = vectorizer.get_feature_names()\n",
    "\n",
    "term_doc_matrix_mod = vectorizer.fit_transform(text_mod)\n",
    "# vocabulary_mod = vectorizer.get_feature_names()\n",
    "\n",
    "# Convert hasil fit_transform dari sparse matrix ke list\n",
    "tfidf_def = flatten(term_doc_matrix_def)\n",
    "tfidf_mod = flatten(term_doc_matrix_mod)\n",
    "\n",
    "# Apply weighting untuk TF-IDF\n",
    "tfposidf_def = postag_weighting(tfidf_def, df['tag_pair'])\n",
    "tfposidf_mod = postag_weighting(tfidf_mod, df['tag_pair_mod'])\n",
    "\n",
    "# Convert TFPOS-IDF ke dataframe\n",
    "# Ini yang dipake buat model\n",
    "df_tfidf_def = pd.DataFrame(tfidf_def)\n",
    "df_tfidf_mod = pd.DataFrame(tfidf_mod)\n",
    "\n",
    "df_tfposidf_def = pd.DataFrame(tfposidf_def)\n",
    "df_tfposidf_mod = pd.DataFrame(tfposidf_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy = 'not majority', random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_def, y_def = ros.fit_resample(tfposidf_def, df['label'])\n",
    "X_mod, y_mod = ros.fit_resample(tfposidf_mod, df['label'])\n",
    "\n",
    "df_ros_def = pd.DataFrame(X_def)\n",
    "ros_label_def = pd.DataFrame(y_def)\n",
    "df_ros_mod = pd.DataFrame(X_mod)\n",
    "ros_label_mod = pd.DataFrame(y_mod)\n",
    "\n",
    "X_def, y_def = ros.fit_resample(tfidf_def, df['label'])\n",
    "X_mod, y_mod = ros.fit_resample(tfidf_mod, df['label'])\n",
    "\n",
    "df_tfidf_ros_def = pd.DataFrame(X_def)\n",
    "tfidf_ros_label_def = pd.DataFrame(y_def)\n",
    "df_tfidf_ros_mod = pd.DataFrame(X_mod)\n",
    "tfidf_ros_label_mod = pd.DataFrame(y_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skenario pengujian\n",
    "1. TFIDF reguler, stopword PySastrawi ==> use df_tfidf_def\n",
    "2. TFIDF reguler, stopword modifikasi PySastrawi ==> use df_tfidf_mod\n",
    "3. TFIDF reguler, stopword PySastrawi, random over-sampling ==> df_tfidf_ros_def\n",
    "4. TFIDF reguler, stopword modifikasi PySastrawi, random over-sampling ==> df_tfidf_ros_mod\n",
    "5. TFPOS-IDF, stopword PySastrawi ==> use df_tfposidf_def\n",
    "6. TFPOS-IDF, stopword modifikasi PySastrawi ==> df_tfposidf_mod\n",
    "7. TFPOS-IDF, stopword PySastrawi, random over-sampling ==> df_ros_def\n",
    "8. TFPOS-IDF, stopword modifikasi PySastrawi, random over-sampling ==> df_ros_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(test, pred):\n",
    "    accuracy = metrics.accuracy_score(test, pred)\n",
    "    precision = metrics.precision_score(test, pred, average='weighted')\n",
    "    recall = metrics.recall_score(test, pred, average='weighted')\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 1 - SVM\n",
    "TFIDF reguler, stopword PySastrawi ==> use df_tfidf_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm1, X_test_svm1, y_train_svm1, y_test_svm1 = train_test_split(df_tfidf_def, \n",
    "                                                                        df['label'], \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=23)\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train_svm1, y_train_svm1)\n",
    "pred_svm = clf_svm.predict(X_test_svm1)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_svm1, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train_svm1, y_train_svm1)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=100, gamma = 0.01)\n",
    "clf_svm.fit(X_train_svm1, y_train_svm1)\n",
    "pred_svm = clf_svm.predict(X_test_svm1)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_svm1, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 1 - NB\n",
    "TFIDF reguler, stopword PySastrawi ==> use df_tfidf_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_nb1, X_test_nb1, y_train_nb1, y_test_nb1 = train_test_split(df_tfidf_def, \n",
    "                                                                    df['label'], \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=23)\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train_nb1, y_train_nb1)\n",
    "pred_nb = clf_nb.predict(X_test_nb1)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_nb1, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train_nb1, y_train_nb1)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.1)\n",
    "clf_nb.fit(X_train_nb1, y_train_nb1)\n",
    "pred_nb = clf_nb.predict(X_test_nb1)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_nb1, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 2 - SVM\n",
    "TFIDF reguler, stopword modifikasi PySastrawi ==> use df_tfidf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_mod, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=100, gamma = 0.01)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 2 - NB\n",
    "TFIDF reguler, stopword modifikasi PySastrawi ==> use df_tfidf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_mod, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.1)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 3 - SVM\n",
    "TFIDF reguler, stopword PySastrawi, random over-sampling  ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm3, X_test_svm3, y_train_svm3, y_test_svm3 = train_test_split(df_tfidf_ros_def, \n",
    "                                                                        tfidf_ros_label_def, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=23)\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train_svm3, y_train_svm3)\n",
    "pred_svm = clf_svm.predict(X_test_svm3)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_svm3, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train_svm3, y_train_svm3)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=10, kernel='linear')\n",
    "clf_svm.fit(X_train_svm3, y_train_svm3)\n",
    "pred_svm = clf_svm.predict(X_test_svm3)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test_svm3, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 3 - NB\n",
    "TFIDF reguler, stopword PySastrawi, random over-sampling  ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_ros_def, tfidf_ros_label_def, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.0)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 4 - SVM\n",
    "TFIDF reguler, stopword modifikasi PySastrawi, random over-sampling  ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_ros_mod, tfidf_ros_label_mod, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=10, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 4 - NB\n",
    "TFIDF reguler, stopword modifikasi PySastrawi, random over-sampling  ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf_ros_def, tfidf_ros_label_def, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.0)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 5 - SVM\n",
    "TFPOS-IDF, stopword PySastrawi ==> use df_tfposidf_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfposidf_def, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 5 - NB\n",
    "TFPOS-IDF, stopword PySastrawi ==> use df_tfposidf_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfposidf_def, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.9)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 6 - SVM\n",
    "TFPOS-IDF, stopword modifikasi PySastrawi ==> df_tfposidf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfposidf_mod, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=100, gamma=0.001)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 6 - NB\n",
    "TFPOS-IDF, stopword modifikasi PySastrawi ==> df_tfposidf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfposidf_mod, df['label'], test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 7 - SVM\n",
    "TFPOS-IDF, stopword PySastrawi, random over-sampling ==> df_ros_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_ros_def, ros_label_def, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 7 - NB\n",
    "TFPOS-IDF, stopword PySastrawi, random over-sampling ==> df_ros_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_ros_def, ros_label_def, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.0)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 8 - SVM\n",
    "TFPOS-IDF, stopword PySastrawi, random over-sampling ==> df_ros_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_ros_mod, ros_label_mod, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_svm = svm.SVC(C=1, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svm_grid_param = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001, 'auto'], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_svm = GridSearchCV(clf_svm, svm_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "print(gs_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=100, kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "pred_svm = clf_svm.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_svm)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skenario 8 - NB\n",
    "TFPOS-IDF, stopword PySastrawi, random over-sampling ==> df_ros_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_ros_mod, ros_label_mod, test_size=0.2, random_state=23)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_grid_param = [ {'alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "                  {'fit_prior' : [True, False]} ]\n",
    "\n",
    "# make a GridSearchCV object\n",
    "gs_nb = GridSearchCV(clf_nb, nb_grid_param, cv = 5, verbose = 1, scoring='f1_micro')\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(gs_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB(alpha=0.0)\n",
    "clf_nb.fit(X_train, y_train)\n",
    "pred_nb = clf_nb.predict(X_test)\n",
    "\n",
    "accuracy, precision, recall, f1_score = get_score(y_test, pred_nb)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
