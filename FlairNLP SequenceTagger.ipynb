{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import UD_INDONESIAN\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 15:35:50,098 Reading data from C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\n",
      "2022-02-07 15:35:50,099 Train: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-train.conllu\n",
      "2022-02-07 15:35:50,099 Dev: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-dev.conllu\n",
      "2022-02-07 15:35:50,101 Test: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-test.conllu\n",
      "Corpus: 4482 train + 559 dev + 557 test sentences\n",
      "2022-02-07 15:35:53,317 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4482/4482 [00:00<00:00, 14781.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 15:35:53,869 Corpus contains the labels: lemma (#97604), upos (#97604), pos (#97604), dependency (#97604), number (#19599), voice (#8616), mood (#8563), prontype (#8489), numtype (#3787), person (#2229), definite (#748), polarity (#483), polite (#215), degree (#176), typo (#74), clusivity (#72), foreign (#72), reflex (#69), abbr (#10)\n",
      "2022-02-07 15:35:53,869 Created (for label 'upos') Dictionary with 18 tags: <unk>, PROPN, AUX, DET, NOUN, PRON, VERB, ADP, PUNCT, ADV, CCONJ, SCONJ, NUM, ADJ, PART, SYM, INTJ, X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 18 tags: <unk>, PROPN, AUX, DET, NOUN, PRON, VERB, ADP, PUNCT, ADV, CCONJ, SCONJ, NUM, ADJ, PART, SYM, INTJ, X\n",
      "2022-02-07 15:36:23,005 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,006 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'id'\n",
      "      (embedding): Embedding(300686, 300)\n",
      "    )\n",
      "    (list_embedding_1): WordEmbeddings(\n",
      "      'id-crawl'\n",
      "      (embedding): Embedding(1000000, 300)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (rnn): LSTM(600, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-02-07 15:36:23,007 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,009 Corpus: \"Corpus: 4482 train + 559 dev + 557 test sentences\"\n",
      "2022-02-07 15:36:23,010 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,011 Parameters:\n",
      "2022-02-07 15:36:23,012  - learning_rate: \"0.1\"\n",
      "2022-02-07 15:36:23,012  - mini_batch_size: \"32\"\n",
      "2022-02-07 15:36:23,013  - patience: \"3\"\n",
      "2022-02-07 15:36:23,015  - anneal_factor: \"0.5\"\n",
      "2022-02-07 15:36:23,016  - max_epochs: \"10\"\n",
      "2022-02-07 15:36:23,016  - shuffle: \"True\"\n",
      "2022-02-07 15:36:23,017  - train_with_dev: \"False\"\n",
      "2022-02-07 15:36:23,018  - batch_growth_annealing: \"False\"\n",
      "2022-02-07 15:36:23,018 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,019 Model training base path: \"resources\\taggers\\example-upos\"\n",
      "2022-02-07 15:36:23,021 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,022 Device: cuda:0\n",
      "2022-02-07 15:36:23,023 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:36:23,023 Embeddings storage mode: cpu\n",
      "2022-02-07 15:36:23,111 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 15:36:32,827 epoch 1 - iter 14/141 - loss 2.72497865 - samples/sec: 46.12 - lr: 0.100000\n",
      "2022-02-07 15:36:37,719 epoch 1 - iter 28/141 - loss 2.33325678 - samples/sec: 91.63 - lr: 0.100000\n",
      "2022-02-07 15:36:42,418 epoch 1 - iter 42/141 - loss 2.08923475 - samples/sec: 95.38 - lr: 0.100000\n",
      "2022-02-07 15:36:47,284 epoch 1 - iter 56/141 - loss 1.91186408 - samples/sec: 92.09 - lr: 0.100000\n",
      "2022-02-07 15:36:52,445 epoch 1 - iter 70/141 - loss 1.77547182 - samples/sec: 86.87 - lr: 0.100000\n",
      "2022-02-07 15:36:57,577 epoch 1 - iter 84/141 - loss 1.66672966 - samples/sec: 87.31 - lr: 0.100000\n",
      "2022-02-07 15:37:02,978 epoch 1 - iter 98/141 - loss 1.57061662 - samples/sec: 82.97 - lr: 0.100000\n",
      "2022-02-07 15:37:08,488 epoch 1 - iter 112/141 - loss 1.49491398 - samples/sec: 81.35 - lr: 0.100000\n",
      "2022-02-07 15:37:13,547 epoch 1 - iter 126/141 - loss 1.42620988 - samples/sec: 88.59 - lr: 0.100000\n",
      "2022-02-07 15:37:18,172 epoch 1 - iter 140/141 - loss 1.36997272 - samples/sec: 96.92 - lr: 0.100000\n",
      "2022-02-07 15:37:18,258 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:37:18,259 EPOCH 1 done: loss 1.3699 - lr 0.1000000\n",
      "2022-02-07 15:37:27,886 DEV : loss 0.5740399360656738 - f1-score (micro avg)  0.8139\n",
      "2022-02-07 15:37:27,929 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:37:27,931 saving best model\n",
      "2022-02-07 15:37:48,278 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:37:52,108 epoch 2 - iter 14/141 - loss 0.75423634 - samples/sec: 119.71 - lr: 0.100000\n",
      "2022-02-07 15:37:55,910 epoch 2 - iter 28/141 - loss 0.72564610 - samples/sec: 117.85 - lr: 0.100000\n",
      "2022-02-07 15:37:59,711 epoch 2 - iter 42/141 - loss 0.71498174 - samples/sec: 117.86 - lr: 0.100000\n",
      "2022-02-07 15:38:03,836 epoch 2 - iter 56/141 - loss 0.71181424 - samples/sec: 108.63 - lr: 0.100000\n",
      "2022-02-07 15:38:07,983 epoch 2 - iter 70/141 - loss 0.70046385 - samples/sec: 108.09 - lr: 0.100000\n",
      "2022-02-07 15:38:12,015 epoch 2 - iter 84/141 - loss 0.68999226 - samples/sec: 111.15 - lr: 0.100000\n",
      "2022-02-07 15:38:15,757 epoch 2 - iter 98/141 - loss 0.67701216 - samples/sec: 119.81 - lr: 0.100000\n",
      "2022-02-07 15:38:19,192 epoch 2 - iter 112/141 - loss 0.66440714 - samples/sec: 130.54 - lr: 0.100000\n",
      "2022-02-07 15:38:22,945 epoch 2 - iter 126/141 - loss 0.65428593 - samples/sec: 119.43 - lr: 0.100000\n",
      "2022-02-07 15:38:26,212 epoch 2 - iter 140/141 - loss 0.64543587 - samples/sec: 137.28 - lr: 0.100000\n",
      "2022-02-07 15:38:26,255 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:38:26,256 EPOCH 2 done: loss 0.6454 - lr 0.1000000\n",
      "2022-02-07 15:38:34,158 DEV : loss 0.3591832220554352 - f1-score (micro avg)  0.8863\n",
      "2022-02-07 15:38:34,198 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:38:34,199 saving best model\n",
      "2022-02-07 15:38:53,518 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:38:57,632 epoch 3 - iter 14/141 - loss 0.53810596 - samples/sec: 108.97 - lr: 0.100000\n",
      "2022-02-07 15:39:01,190 epoch 3 - iter 28/141 - loss 0.53877866 - samples/sec: 126.01 - lr: 0.100000\n",
      "2022-02-07 15:39:04,734 epoch 3 - iter 42/141 - loss 0.53064809 - samples/sec: 126.50 - lr: 0.100000\n",
      "2022-02-07 15:39:08,906 epoch 3 - iter 56/141 - loss 0.53333304 - samples/sec: 107.41 - lr: 0.100000\n",
      "2022-02-07 15:39:12,875 epoch 3 - iter 70/141 - loss 0.52771963 - samples/sec: 112.93 - lr: 0.100000\n",
      "2022-02-07 15:39:16,814 epoch 3 - iter 84/141 - loss 0.52730616 - samples/sec: 113.85 - lr: 0.100000\n",
      "2022-02-07 15:39:20,574 epoch 3 - iter 98/141 - loss 0.52343750 - samples/sec: 119.20 - lr: 0.100000\n",
      "2022-02-07 15:39:24,217 epoch 3 - iter 112/141 - loss 0.51866858 - samples/sec: 123.08 - lr: 0.100000\n",
      "2022-02-07 15:39:28,003 epoch 3 - iter 126/141 - loss 0.51591955 - samples/sec: 118.36 - lr: 0.100000\n",
      "2022-02-07 15:39:31,295 epoch 3 - iter 140/141 - loss 0.51330297 - samples/sec: 136.14 - lr: 0.100000\n",
      "2022-02-07 15:39:31,354 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:39:31,356 EPOCH 3 done: loss 0.5133 - lr 0.1000000\n",
      "2022-02-07 15:39:39,137 DEV : loss 0.3888123035430908 - f1-score (micro avg)  0.8597\n",
      "2022-02-07 15:39:39,175 BAD EPOCHS (no improvement): 1\n",
      "2022-02-07 15:39:39,176 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:39:43,016 epoch 4 - iter 14/141 - loss 0.49152149 - samples/sec: 116.77 - lr: 0.100000\n",
      "2022-02-07 15:39:46,776 epoch 4 - iter 28/141 - loss 0.48642294 - samples/sec: 119.18 - lr: 0.100000\n",
      "2022-02-07 15:39:50,086 epoch 4 - iter 42/141 - loss 0.48165317 - samples/sec: 135.37 - lr: 0.100000\n",
      "2022-02-07 15:39:53,810 epoch 4 - iter 56/141 - loss 0.48225117 - samples/sec: 120.35 - lr: 0.100000\n",
      "2022-02-07 15:39:57,725 epoch 4 - iter 70/141 - loss 0.47763191 - samples/sec: 114.44 - lr: 0.100000\n",
      "2022-02-07 15:40:01,651 epoch 4 - iter 84/141 - loss 0.47432837 - samples/sec: 114.19 - lr: 0.100000\n",
      "2022-02-07 15:40:05,255 epoch 4 - iter 98/141 - loss 0.47177236 - samples/sec: 124.35 - lr: 0.100000\n",
      "2022-02-07 15:40:09,465 epoch 4 - iter 112/141 - loss 0.47007352 - samples/sec: 106.51 - lr: 0.100000\n",
      "2022-02-07 15:40:14,110 epoch 4 - iter 126/141 - loss 0.47168105 - samples/sec: 96.48 - lr: 0.100000\n",
      "2022-02-07 15:40:17,591 epoch 4 - iter 140/141 - loss 0.47046080 - samples/sec: 128.73 - lr: 0.100000\n",
      "2022-02-07 15:40:17,664 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:40:17,665 EPOCH 4 done: loss 0.4704 - lr 0.1000000\n",
      "2022-02-07 15:40:26,504 DEV : loss 0.2786737382411957 - f1-score (micro avg)  0.9063\n",
      "2022-02-07 15:40:26,545 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:40:26,547 saving best model\n",
      "2022-02-07 15:40:42,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:40:46,509 epoch 5 - iter 14/141 - loss 0.44925822 - samples/sec: 120.65 - lr: 0.100000\n",
      "2022-02-07 15:40:50,413 epoch 5 - iter 28/141 - loss 0.45855483 - samples/sec: 114.78 - lr: 0.100000\n",
      "2022-02-07 15:40:53,981 epoch 5 - iter 42/141 - loss 0.44685370 - samples/sec: 125.61 - lr: 0.100000\n",
      "2022-02-07 15:40:58,108 epoch 5 - iter 56/141 - loss 0.44727052 - samples/sec: 108.59 - lr: 0.100000\n",
      "2022-02-07 15:41:02,115 epoch 5 - iter 70/141 - loss 0.44470873 - samples/sec: 111.84 - lr: 0.100000\n",
      "2022-02-07 15:41:05,647 epoch 5 - iter 84/141 - loss 0.44468094 - samples/sec: 126.87 - lr: 0.100000\n",
      "2022-02-07 15:41:09,226 epoch 5 - iter 98/141 - loss 0.44467626 - samples/sec: 125.23 - lr: 0.100000\n",
      "2022-02-07 15:41:12,612 epoch 5 - iter 112/141 - loss 0.44415171 - samples/sec: 132.36 - lr: 0.100000\n",
      "2022-02-07 15:41:16,280 epoch 5 - iter 126/141 - loss 0.43995586 - samples/sec: 122.18 - lr: 0.100000\n",
      "2022-02-07 15:41:20,068 epoch 5 - iter 140/141 - loss 0.44087298 - samples/sec: 118.30 - lr: 0.100000\n",
      "2022-02-07 15:41:20,202 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:41:20,203 EPOCH 5 done: loss 0.4409 - lr 0.1000000\n",
      "2022-02-07 15:41:28,277 DEV : loss 0.2874363362789154 - f1-score (micro avg)  0.9011\n",
      "2022-02-07 15:41:28,315 BAD EPOCHS (no improvement): 1\n",
      "2022-02-07 15:41:28,317 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:41:32,360 epoch 6 - iter 14/141 - loss 0.41069645 - samples/sec: 110.87 - lr: 0.100000\n",
      "2022-02-07 15:41:35,874 epoch 6 - iter 28/141 - loss 0.41979985 - samples/sec: 127.54 - lr: 0.100000\n",
      "2022-02-07 15:41:39,512 epoch 6 - iter 42/141 - loss 0.42231876 - samples/sec: 123.21 - lr: 0.100000\n",
      "2022-02-07 15:41:43,191 epoch 6 - iter 56/141 - loss 0.42734312 - samples/sec: 121.85 - lr: 0.100000\n",
      "2022-02-07 15:41:46,983 epoch 6 - iter 70/141 - loss 0.42058740 - samples/sec: 118.20 - lr: 0.100000\n",
      "2022-02-07 15:41:50,443 epoch 6 - iter 84/141 - loss 0.41734029 - samples/sec: 129.55 - lr: 0.100000\n",
      "2022-02-07 15:41:54,275 epoch 6 - iter 98/141 - loss 0.41790720 - samples/sec: 116.92 - lr: 0.100000\n",
      "2022-02-07 15:41:57,814 epoch 6 - iter 112/141 - loss 0.41764954 - samples/sec: 126.62 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 15:42:01,612 epoch 6 - iter 126/141 - loss 0.41870714 - samples/sec: 118.00 - lr: 0.100000\n",
      "2022-02-07 15:42:05,273 epoch 6 - iter 140/141 - loss 0.41911626 - samples/sec: 122.39 - lr: 0.100000\n",
      "2022-02-07 15:42:05,344 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:42:05,345 EPOCH 6 done: loss 0.4192 - lr 0.1000000\n",
      "2022-02-07 15:42:13,920 DEV : loss 0.25320637226104736 - f1-score (micro avg)  0.9153\n",
      "2022-02-07 15:42:13,959 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:42:13,961 saving best model\n",
      "2022-02-07 15:42:33,411 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:42:37,236 epoch 7 - iter 14/141 - loss 0.38650608 - samples/sec: 117.30 - lr: 0.100000\n",
      "2022-02-07 15:42:40,973 epoch 7 - iter 28/141 - loss 0.39770354 - samples/sec: 119.93 - lr: 0.100000\n",
      "2022-02-07 15:42:44,828 epoch 7 - iter 42/141 - loss 0.40178388 - samples/sec: 116.24 - lr: 0.100000\n",
      "2022-02-07 15:42:48,869 epoch 7 - iter 56/141 - loss 0.40289789 - samples/sec: 110.91 - lr: 0.100000\n",
      "2022-02-07 15:42:52,612 epoch 7 - iter 70/141 - loss 0.40318372 - samples/sec: 119.79 - lr: 0.100000\n",
      "2022-02-07 15:42:56,593 epoch 7 - iter 84/141 - loss 0.40562368 - samples/sec: 112.56 - lr: 0.100000\n",
      "2022-02-07 15:43:00,552 epoch 7 - iter 98/141 - loss 0.40730273 - samples/sec: 113.26 - lr: 0.100000\n",
      "2022-02-07 15:43:04,152 epoch 7 - iter 112/141 - loss 0.40872560 - samples/sec: 124.46 - lr: 0.100000\n",
      "2022-02-07 15:43:07,771 epoch 7 - iter 126/141 - loss 0.40518926 - samples/sec: 123.84 - lr: 0.100000\n",
      "2022-02-07 15:43:11,599 epoch 7 - iter 140/141 - loss 0.40655004 - samples/sec: 117.08 - lr: 0.100000\n",
      "2022-02-07 15:43:11,650 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:43:11,652 EPOCH 7 done: loss 0.4065 - lr 0.1000000\n",
      "2022-02-07 15:43:19,777 DEV : loss 0.24597479403018951 - f1-score (micro avg)  0.9201\n",
      "2022-02-07 15:43:19,814 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:43:19,816 saving best model\n",
      "2022-02-07 15:43:35,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:43:39,156 epoch 8 - iter 14/141 - loss 0.39422623 - samples/sec: 122.76 - lr: 0.100000\n",
      "2022-02-07 15:43:42,955 epoch 8 - iter 28/141 - loss 0.39007302 - samples/sec: 117.95 - lr: 0.100000\n",
      "2022-02-07 15:43:46,789 epoch 8 - iter 42/141 - loss 0.38986738 - samples/sec: 116.99 - lr: 0.100000\n",
      "2022-02-07 15:43:50,572 epoch 8 - iter 56/141 - loss 0.38357701 - samples/sec: 118.44 - lr: 0.100000\n",
      "2022-02-07 15:43:54,454 epoch 8 - iter 70/141 - loss 0.38822376 - samples/sec: 115.47 - lr: 0.100000\n",
      "2022-02-07 15:43:58,047 epoch 8 - iter 84/141 - loss 0.38662134 - samples/sec: 124.68 - lr: 0.100000\n",
      "2022-02-07 15:44:01,766 epoch 8 - iter 98/141 - loss 0.38908267 - samples/sec: 120.49 - lr: 0.100000\n",
      "2022-02-07 15:44:05,611 epoch 8 - iter 112/141 - loss 0.39014803 - samples/sec: 116.58 - lr: 0.100000\n",
      "2022-02-07 15:44:09,505 epoch 8 - iter 126/141 - loss 0.39453188 - samples/sec: 115.09 - lr: 0.100000\n",
      "2022-02-07 15:44:13,730 epoch 8 - iter 140/141 - loss 0.39465294 - samples/sec: 106.10 - lr: 0.100000\n",
      "2022-02-07 15:44:13,805 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:44:13,807 EPOCH 8 done: loss 0.3946 - lr 0.1000000\n",
      "2022-02-07 15:44:21,724 DEV : loss 0.24322865903377533 - f1-score (micro avg)  0.9204\n",
      "2022-02-07 15:44:21,763 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:44:21,764 saving best model\n",
      "2022-02-07 15:44:41,172 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:44:44,799 epoch 9 - iter 14/141 - loss 0.39499379 - samples/sec: 123.59 - lr: 0.100000\n",
      "2022-02-07 15:44:48,779 epoch 9 - iter 28/141 - loss 0.39488992 - samples/sec: 112.58 - lr: 0.100000\n",
      "2022-02-07 15:44:52,592 epoch 9 - iter 42/141 - loss 0.39588154 - samples/sec: 117.56 - lr: 0.100000\n",
      "2022-02-07 15:44:56,729 epoch 9 - iter 56/141 - loss 0.39270352 - samples/sec: 108.31 - lr: 0.100000\n",
      "2022-02-07 15:45:00,235 epoch 9 - iter 70/141 - loss 0.39456374 - samples/sec: 127.82 - lr: 0.100000\n",
      "2022-02-07 15:45:04,111 epoch 9 - iter 84/141 - loss 0.39359753 - samples/sec: 115.64 - lr: 0.100000\n",
      "2022-02-07 15:45:07,886 epoch 9 - iter 98/141 - loss 0.39224479 - samples/sec: 118.69 - lr: 0.100000\n",
      "2022-02-07 15:45:11,375 epoch 9 - iter 112/141 - loss 0.39302278 - samples/sec: 128.48 - lr: 0.100000\n",
      "2022-02-07 15:45:15,126 epoch 9 - iter 126/141 - loss 0.39166207 - samples/sec: 119.51 - lr: 0.100000\n",
      "2022-02-07 15:45:18,871 epoch 9 - iter 140/141 - loss 0.38999760 - samples/sec: 119.67 - lr: 0.100000\n",
      "2022-02-07 15:45:18,938 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:45:18,939 EPOCH 9 done: loss 0.3899 - lr 0.1000000\n",
      "2022-02-07 15:45:27,601 DEV : loss 0.23382115364074707 - f1-score (micro avg)  0.9239\n",
      "2022-02-07 15:45:27,641 BAD EPOCHS (no improvement): 0\n",
      "2022-02-07 15:45:27,642 saving best model\n",
      "2022-02-07 15:45:43,975 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:45:47,902 epoch 10 - iter 14/141 - loss 0.39097702 - samples/sec: 114.15 - lr: 0.100000\n",
      "2022-02-07 15:45:51,466 epoch 10 - iter 28/141 - loss 0.37059396 - samples/sec: 125.76 - lr: 0.100000\n",
      "2022-02-07 15:45:55,162 epoch 10 - iter 42/141 - loss 0.37595021 - samples/sec: 121.37 - lr: 0.100000\n",
      "2022-02-07 15:45:58,925 epoch 10 - iter 56/141 - loss 0.37780839 - samples/sec: 119.09 - lr: 0.100000\n",
      "2022-02-07 15:46:03,212 epoch 10 - iter 70/141 - loss 0.37647104 - samples/sec: 104.55 - lr: 0.100000\n",
      "2022-02-07 15:46:07,426 epoch 10 - iter 84/141 - loss 0.37705811 - samples/sec: 106.34 - lr: 0.100000\n",
      "2022-02-07 15:46:11,216 epoch 10 - iter 98/141 - loss 0.37817773 - samples/sec: 118.28 - lr: 0.100000\n",
      "2022-02-07 15:46:14,866 epoch 10 - iter 112/141 - loss 0.37813318 - samples/sec: 122.77 - lr: 0.100000\n",
      "2022-02-07 15:46:18,621 epoch 10 - iter 126/141 - loss 0.37873722 - samples/sec: 119.36 - lr: 0.100000\n",
      "2022-02-07 15:46:22,186 epoch 10 - iter 140/141 - loss 0.37923426 - samples/sec: 125.71 - lr: 0.100000\n",
      "2022-02-07 15:46:22,270 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:46:22,271 EPOCH 10 done: loss 0.3792 - lr 0.1000000\n",
      "2022-02-07 15:46:30,095 DEV : loss 0.24327149987220764 - f1-score (micro avg)  0.9146\n",
      "2022-02-07 15:46:30,134 BAD EPOCHS (no improvement): 1\n",
      "2022-02-07 15:46:49,827 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-07 15:46:49,830 loading file resources\\taggers\\example-upos\\best-model.pt\n",
      "2022-02-07 15:47:00,714 0.9279\t0.9279\t0.9279\t0.9279\n",
      "2022-02-07 15:47:00,716 \n",
      "Results:\n",
      "- F-score (micro) 0.9279\n",
      "- F-score (macro) 0.8614\n",
      "- Accuracy 0.9279\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN     0.9037    0.8897    0.8966      2511\n",
      "       PROPN     0.9069    0.9371    0.9217      2162\n",
      "       PUNCT     0.9975    0.9994    0.9985      1623\n",
      "        VERB     0.9485    0.9342    0.9413      1261\n",
      "         ADP     0.9434    0.9434    0.9434      1114\n",
      "        PRON     0.9344    0.9736    0.9536       644\n",
      "         ADJ     0.8455    0.7623    0.8017       488\n",
      "         NUM     0.9867    0.9635    0.9750       384\n",
      "       CCONJ     0.9863    0.9945    0.9904       362\n",
      "         ADV     0.8140    0.8092    0.8116       346\n",
      "         DET     0.9531    0.8944    0.9228       341\n",
      "         AUX     0.9573    0.9782    0.9676       229\n",
      "       SCONJ     0.7671    0.8660    0.8136       194\n",
      "        PART     0.9053    0.9663    0.9348        89\n",
      "         SYM     1.0000    0.8333    0.9091         6\n",
      "           X     0.0000    0.0000    0.0000         2\n",
      "\n",
      "   micro avg     0.9279    0.9279    0.9279     11756\n",
      "   macro avg     0.8656    0.8591    0.8614     11756\n",
      "weighted avg     0.9278    0.9279    0.9276     11756\n",
      " samples avg     0.9279    0.9279    0.9279     11756\n",
      "\n",
      "2022-02-07 15:47:00,716 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9278666212997617,\n",
       " 'dev_score_history': [0.8139167522312614,\n",
       "  0.8863438906879393,\n",
       "  0.8597267198483534,\n",
       "  0.9063265144933259,\n",
       "  0.901113656109312,\n",
       "  0.9153305426111682,\n",
       "  0.9201484874812416,\n",
       "  0.9203854355896058,\n",
       "  0.9239396572150699,\n",
       "  0.9146196982860754],\n",
       " 'train_loss_history': [1.3698801693967715,\n",
       "  0.6453987349203818,\n",
       "  0.5132885682749448,\n",
       "  0.47043550571923004,\n",
       "  0.44089727635061365,\n",
       "  0.41920872937606535,\n",
       "  0.40651557005935457,\n",
       "  0.39462733406506817,\n",
       "  0.38993566641333277,\n",
       "  0.3791860742484159],\n",
       " 'dev_loss_history': [tensor(0.5740, device='cuda:0'),\n",
       "  tensor(0.3592, device='cuda:0'),\n",
       "  tensor(0.3888, device='cuda:0'),\n",
       "  tensor(0.2787, device='cuda:0'),\n",
       "  tensor(0.2874, device='cuda:0'),\n",
       "  tensor(0.2532, device='cuda:0'),\n",
       "  tensor(0.2460, device='cuda:0'),\n",
       "  tensor(0.2432, device='cuda:0'),\n",
       "  tensor(0.2338, device='cuda:0'),\n",
       "  tensor(0.2433, device='cuda:0')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "corpus = UD_INDONESIAN()\n",
    "print(corpus)\n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'upos'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('id'),\n",
    "    WordEmbeddings('id-crawl')\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-upos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
