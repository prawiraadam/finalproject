{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from flair.datasets import UD_INDONESIAN\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 18:58:32,463 Reading data from C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\n",
      "2022-01-28 18:58:32,464 Train: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-train.conllu\n",
      "2022-01-28 18:58:32,465 Dev: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-dev.conllu\n",
      "2022-01-28 18:58:32,465 Test: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-test.conllu\n",
      "Corpus: 4482 train + 559 dev + 557 test sentences\n",
      "2022-01-28 18:58:35,229 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4482/4482 [00:00<00:00, 18268.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 18:58:35,479 Corpus contains the labels: lemma (#97604), upos (#97604), pos (#97604), dependency (#97604), number (#19599), voice (#8616), mood (#8563), prontype (#8489), numtype (#3787), person (#2229), definite (#748), polarity (#483), polite (#215), degree (#176), typo (#74), clusivity (#72), foreign (#72), reflex (#69), abbr (#10)\n",
      "2022-01-28 18:58:35,480 Created (for label 'upos') Dictionary with 18 tags: <unk>, PROPN, AUX, DET, NOUN, PRON, VERB, ADP, PUNCT, ADV, CCONJ, SCONJ, NUM, ADJ, PART, SYM, INTJ, X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 18 tags: <unk>, PROPN, AUX, DET, NOUN, PRON, VERB, ADP, PUNCT, ADV, CCONJ, SCONJ, NUM, ADJ, PART, SYM, INTJ, X\n",
      "2022-01-28 18:58:43,195 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,196 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'id'\n",
      "      (embedding): Embedding(300686, 300)\n",
      "    )\n",
      "    (list_embedding_1): WordEmbeddings(\n",
      "      'id-crawl'\n",
      "      (embedding): Embedding(1000000, 300)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (rnn): LSTM(600, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-01-28 18:58:43,197 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,198 Corpus: \"Corpus: 4482 train + 559 dev + 557 test sentences\"\n",
      "2022-01-28 18:58:43,199 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,200 Parameters:\n",
      "2022-01-28 18:58:43,201  - learning_rate: \"0.1\"\n",
      "2022-01-28 18:58:43,202  - mini_batch_size: \"32\"\n",
      "2022-01-28 18:58:43,204  - patience: \"3\"\n",
      "2022-01-28 18:58:43,205  - anneal_factor: \"0.5\"\n",
      "2022-01-28 18:58:43,205  - max_epochs: \"10\"\n",
      "2022-01-28 18:58:43,206  - shuffle: \"True\"\n",
      "2022-01-28 18:58:43,207  - train_with_dev: \"False\"\n",
      "2022-01-28 18:58:43,208  - batch_growth_annealing: \"False\"\n",
      "2022-01-28 18:58:43,210 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,211 Model training base path: \"resources\\taggers\\example-upos\"\n",
      "2022-01-28 18:58:43,212 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,215 Device: cuda:0\n",
      "2022-01-28 18:58:43,218 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:58:43,219 Embeddings storage mode: cpu\n",
      "2022-01-28 18:58:43,268 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 18:58:47,673 epoch 1 - iter 14/141 - loss 2.73815282 - samples/sec: 101.74 - lr: 0.100000\n",
      "2022-01-28 18:58:52,107 epoch 1 - iter 28/141 - loss 2.42652344 - samples/sec: 101.06 - lr: 0.100000\n",
      "2022-01-28 18:58:56,108 epoch 1 - iter 42/141 - loss 2.19704371 - samples/sec: 112.05 - lr: 0.100000\n",
      "2022-01-28 18:59:00,220 epoch 1 - iter 56/141 - loss 2.01685630 - samples/sec: 108.99 - lr: 0.100000\n",
      "2022-01-28 18:59:04,601 epoch 1 - iter 70/141 - loss 1.87474423 - samples/sec: 102.37 - lr: 0.100000\n",
      "2022-01-28 18:59:09,002 epoch 1 - iter 84/141 - loss 1.76517457 - samples/sec: 101.84 - lr: 0.100000\n",
      "2022-01-28 18:59:13,574 epoch 1 - iter 98/141 - loss 1.66456958 - samples/sec: 98.01 - lr: 0.100000\n",
      "2022-01-28 18:59:17,887 epoch 1 - iter 112/141 - loss 1.58436254 - samples/sec: 103.88 - lr: 0.100000\n",
      "2022-01-28 18:59:22,261 epoch 1 - iter 126/141 - loss 1.51030206 - samples/sec: 102.50 - lr: 0.100000\n",
      "2022-01-28 18:59:26,126 epoch 1 - iter 140/141 - loss 1.44741859 - samples/sec: 115.98 - lr: 0.100000\n",
      "2022-01-28 18:59:26,162 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:59:26,163 EPOCH 1 done: loss 1.4473 - lr 0.1000000\n",
      "2022-01-28 18:59:36,372 DEV : loss 0.6211581826210022 - f1-score (micro avg)  0.8069\n",
      "2022-01-28 18:59:36,412 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 18:59:36,414 saving best model\n",
      "2022-01-28 18:59:51,869 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 18:59:55,304 epoch 2 - iter 14/141 - loss 0.80074724 - samples/sec: 130.52 - lr: 0.100000\n",
      "2022-01-28 18:59:58,549 epoch 2 - iter 28/141 - loss 0.77481838 - samples/sec: 138.09 - lr: 0.100000\n",
      "2022-01-28 19:00:01,327 epoch 2 - iter 42/141 - loss 0.76787760 - samples/sec: 161.38 - lr: 0.100000\n",
      "2022-01-28 19:00:04,735 epoch 2 - iter 56/141 - loss 0.74895267 - samples/sec: 131.50 - lr: 0.100000\n",
      "2022-01-28 19:00:07,721 epoch 2 - iter 70/141 - loss 0.72668359 - samples/sec: 150.13 - lr: 0.100000\n",
      "2022-01-28 19:00:10,799 epoch 2 - iter 84/141 - loss 0.71761670 - samples/sec: 145.60 - lr: 0.100000\n",
      "2022-01-28 19:00:14,158 epoch 2 - iter 98/141 - loss 0.70864929 - samples/sec: 133.45 - lr: 0.100000\n",
      "2022-01-28 19:00:17,350 epoch 2 - iter 112/141 - loss 0.69743658 - samples/sec: 140.42 - lr: 0.100000\n",
      "2022-01-28 19:00:20,544 epoch 2 - iter 126/141 - loss 0.68588726 - samples/sec: 140.35 - lr: 0.100000\n",
      "2022-01-28 19:00:23,850 epoch 2 - iter 140/141 - loss 0.67383220 - samples/sec: 135.54 - lr: 0.100000\n",
      "2022-01-28 19:00:23,934 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:00:23,935 EPOCH 2 done: loss 0.6739 - lr 0.1000000\n",
      "2022-01-28 19:00:31,305 DEV : loss 0.3888773024082184 - f1-score (micro avg)  0.8682\n",
      "2022-01-28 19:00:31,344 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:00:31,346 saving best model\n",
      "2022-01-28 19:00:47,560 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:00:50,414 epoch 3 - iter 14/141 - loss 0.56481969 - samples/sec: 157.10 - lr: 0.100000\n",
      "2022-01-28 19:00:53,872 epoch 3 - iter 28/141 - loss 0.56528600 - samples/sec: 129.60 - lr: 0.100000\n",
      "2022-01-28 19:00:57,025 epoch 3 - iter 42/141 - loss 0.56362443 - samples/sec: 142.24 - lr: 0.100000\n",
      "2022-01-28 19:01:00,533 epoch 3 - iter 56/141 - loss 0.55582546 - samples/sec: 127.74 - lr: 0.100000\n",
      "2022-01-28 19:01:03,896 epoch 3 - iter 70/141 - loss 0.54486070 - samples/sec: 133.33 - lr: 0.100000\n",
      "2022-01-28 19:01:07,029 epoch 3 - iter 84/141 - loss 0.53659013 - samples/sec: 143.09 - lr: 0.100000\n",
      "2022-01-28 19:01:10,378 epoch 3 - iter 98/141 - loss 0.53196578 - samples/sec: 133.89 - lr: 0.100000\n",
      "2022-01-28 19:01:13,498 epoch 3 - iter 112/141 - loss 0.52932027 - samples/sec: 143.74 - lr: 0.100000\n",
      "2022-01-28 19:01:16,809 epoch 3 - iter 126/141 - loss 0.52963442 - samples/sec: 135.34 - lr: 0.100000\n",
      "2022-01-28 19:01:20,410 epoch 3 - iter 140/141 - loss 0.52573410 - samples/sec: 124.46 - lr: 0.100000\n",
      "2022-01-28 19:01:20,460 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:01:20,461 EPOCH 3 done: loss 0.5257 - lr 0.1000000\n",
      "2022-01-28 19:01:27,530 DEV : loss 0.3303244411945343 - f1-score (micro avg)  0.8908\n",
      "2022-01-28 19:01:27,569 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:01:27,571 saving best model\n",
      "2022-01-28 19:01:43,062 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:01:46,312 epoch 4 - iter 14/141 - loss 0.48879112 - samples/sec: 137.99 - lr: 0.100000\n",
      "2022-01-28 19:01:49,712 epoch 4 - iter 28/141 - loss 0.47719701 - samples/sec: 131.77 - lr: 0.100000\n",
      "2022-01-28 19:01:53,048 epoch 4 - iter 42/141 - loss 0.48620559 - samples/sec: 134.41 - lr: 0.100000\n",
      "2022-01-28 19:01:56,236 epoch 4 - iter 56/141 - loss 0.48641908 - samples/sec: 140.59 - lr: 0.100000\n",
      "2022-01-28 19:01:59,371 epoch 4 - iter 70/141 - loss 0.48119068 - samples/sec: 142.97 - lr: 0.100000\n",
      "2022-01-28 19:02:02,579 epoch 4 - iter 84/141 - loss 0.47887198 - samples/sec: 139.72 - lr: 0.100000\n",
      "2022-01-28 19:02:05,886 epoch 4 - iter 98/141 - loss 0.47820019 - samples/sec: 135.50 - lr: 0.100000\n",
      "2022-01-28 19:02:08,940 epoch 4 - iter 112/141 - loss 0.47538068 - samples/sec: 146.74 - lr: 0.100000\n",
      "2022-01-28 19:02:12,057 epoch 4 - iter 126/141 - loss 0.47306021 - samples/sec: 143.83 - lr: 0.100000\n",
      "2022-01-28 19:02:15,374 epoch 4 - iter 140/141 - loss 0.47100909 - samples/sec: 135.12 - lr: 0.100000\n",
      "2022-01-28 19:02:15,439 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:02:15,440 EPOCH 4 done: loss 0.4710 - lr 0.1000000\n",
      "2022-01-28 19:02:23,686 DEV : loss 0.27696895599365234 - f1-score (micro avg)  0.9098\n",
      "2022-01-28 19:02:23,725 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:02:23,727 saving best model\n",
      "2022-01-28 19:02:40,389 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:02:44,150 epoch 5 - iter 14/141 - loss 0.46278273 - samples/sec: 119.18 - lr: 0.100000\n",
      "2022-01-28 19:02:47,918 epoch 5 - iter 28/141 - loss 0.44875208 - samples/sec: 118.96 - lr: 0.100000\n",
      "2022-01-28 19:02:51,212 epoch 5 - iter 42/141 - loss 0.44873033 - samples/sec: 136.04 - lr: 0.100000\n",
      "2022-01-28 19:02:54,381 epoch 5 - iter 56/141 - loss 0.45202890 - samples/sec: 141.48 - lr: 0.100000\n",
      "2022-01-28 19:02:57,732 epoch 5 - iter 70/141 - loss 0.44388545 - samples/sec: 133.75 - lr: 0.100000\n",
      "2022-01-28 19:03:00,980 epoch 5 - iter 84/141 - loss 0.44296403 - samples/sec: 138.05 - lr: 0.100000\n",
      "2022-01-28 19:03:04,804 epoch 5 - iter 98/141 - loss 0.44370382 - samples/sec: 117.22 - lr: 0.100000\n",
      "2022-01-28 19:03:08,053 epoch 5 - iter 112/141 - loss 0.44180595 - samples/sec: 137.92 - lr: 0.100000\n",
      "2022-01-28 19:03:11,622 epoch 5 - iter 126/141 - loss 0.44161112 - samples/sec: 125.58 - lr: 0.100000\n",
      "2022-01-28 19:03:15,023 epoch 5 - iter 140/141 - loss 0.44140680 - samples/sec: 131.81 - lr: 0.100000\n",
      "2022-01-28 19:03:15,095 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:03:15,096 EPOCH 5 done: loss 0.4414 - lr 0.1000000\n",
      "2022-01-28 19:03:22,536 DEV : loss 0.25653693079948425 - f1-score (micro avg)  0.9125\n",
      "2022-01-28 19:03:22,575 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:03:22,577 saving best model\n",
      "2022-01-28 19:03:37,453 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:03:40,769 epoch 6 - iter 14/141 - loss 0.40016486 - samples/sec: 135.22 - lr: 0.100000\n",
      "2022-01-28 19:03:44,265 epoch 6 - iter 28/141 - loss 0.41341566 - samples/sec: 128.25 - lr: 0.100000\n",
      "2022-01-28 19:03:47,645 epoch 6 - iter 42/141 - loss 0.41863286 - samples/sec: 132.59 - lr: 0.100000\n",
      "2022-01-28 19:03:51,265 epoch 6 - iter 56/141 - loss 0.42123060 - samples/sec: 123.85 - lr: 0.100000\n",
      "2022-01-28 19:03:54,977 epoch 6 - iter 70/141 - loss 0.42209476 - samples/sec: 120.75 - lr: 0.100000\n",
      "2022-01-28 19:03:58,687 epoch 6 - iter 84/141 - loss 0.41935718 - samples/sec: 120.78 - lr: 0.100000\n",
      "2022-01-28 19:04:01,960 epoch 6 - iter 98/141 - loss 0.42193658 - samples/sec: 136.95 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 19:04:05,687 epoch 6 - iter 112/141 - loss 0.41950653 - samples/sec: 120.23 - lr: 0.100000\n",
      "2022-01-28 19:04:08,966 epoch 6 - iter 126/141 - loss 0.41980740 - samples/sec: 136.66 - lr: 0.100000\n",
      "2022-01-28 19:04:12,359 epoch 6 - iter 140/141 - loss 0.41910188 - samples/sec: 132.12 - lr: 0.100000\n",
      "2022-01-28 19:04:12,430 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:04:12,432 EPOCH 6 done: loss 0.4190 - lr 0.1000000\n",
      "2022-01-28 19:04:19,799 DEV : loss 0.24810823798179626 - f1-score (micro avg)  0.9161\n",
      "2022-01-28 19:04:19,839 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:04:19,841 saving best model\n",
      "2022-01-28 19:04:36,412 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:04:39,728 epoch 7 - iter 14/141 - loss 0.42552622 - samples/sec: 135.22 - lr: 0.100000\n",
      "2022-01-28 19:04:43,086 epoch 7 - iter 28/141 - loss 0.40304664 - samples/sec: 133.53 - lr: 0.100000\n",
      "2022-01-28 19:04:46,880 epoch 7 - iter 42/141 - loss 0.41104128 - samples/sec: 118.16 - lr: 0.100000\n",
      "2022-01-28 19:04:50,298 epoch 7 - iter 56/141 - loss 0.41162590 - samples/sec: 131.11 - lr: 0.100000\n",
      "2022-01-28 19:04:53,713 epoch 7 - iter 70/141 - loss 0.40934749 - samples/sec: 131.27 - lr: 0.100000\n",
      "2022-01-28 19:04:57,017 epoch 7 - iter 84/141 - loss 0.41228730 - samples/sec: 135.63 - lr: 0.100000\n",
      "2022-01-28 19:05:00,453 epoch 7 - iter 98/141 - loss 0.41423854 - samples/sec: 130.43 - lr: 0.100000\n",
      "2022-01-28 19:05:04,008 epoch 7 - iter 112/141 - loss 0.41440342 - samples/sec: 126.07 - lr: 0.100000\n",
      "2022-01-28 19:05:07,218 epoch 7 - iter 126/141 - loss 0.41275765 - samples/sec: 139.59 - lr: 0.100000\n",
      "2022-01-28 19:05:10,470 epoch 7 - iter 140/141 - loss 0.41046184 - samples/sec: 137.83 - lr: 0.100000\n",
      "2022-01-28 19:05:10,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:05:10,535 EPOCH 7 done: loss 0.4104 - lr 0.1000000\n",
      "2022-01-28 19:05:18,216 DEV : loss 0.2527296841144562 - f1-score (micro avg)  0.9169\n",
      "2022-01-28 19:05:18,256 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:05:18,258 saving best model\n",
      "2022-01-28 19:05:33,448 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:05:36,857 epoch 8 - iter 14/141 - loss 0.38715363 - samples/sec: 131.61 - lr: 0.100000\n",
      "2022-01-28 19:05:40,025 epoch 8 - iter 28/141 - loss 0.39310492 - samples/sec: 141.61 - lr: 0.100000\n",
      "2022-01-28 19:05:43,417 epoch 8 - iter 42/141 - loss 0.39890127 - samples/sec: 132.16 - lr: 0.100000\n",
      "2022-01-28 19:05:46,757 epoch 8 - iter 56/141 - loss 0.39936775 - samples/sec: 134.21 - lr: 0.100000\n",
      "2022-01-28 19:05:50,054 epoch 8 - iter 70/141 - loss 0.39653575 - samples/sec: 135.96 - lr: 0.100000\n",
      "2022-01-28 19:05:53,709 epoch 8 - iter 84/141 - loss 0.39539419 - samples/sec: 122.66 - lr: 0.100000\n",
      "2022-01-28 19:05:57,559 epoch 8 - iter 98/141 - loss 0.39633217 - samples/sec: 116.43 - lr: 0.100000\n",
      "2022-01-28 19:06:00,862 epoch 8 - iter 112/141 - loss 0.39585985 - samples/sec: 135.75 - lr: 0.100000\n",
      "2022-01-28 19:06:04,139 epoch 8 - iter 126/141 - loss 0.39470149 - samples/sec: 136.78 - lr: 0.100000\n",
      "2022-01-28 19:06:07,556 epoch 8 - iter 140/141 - loss 0.39548723 - samples/sec: 131.19 - lr: 0.100000\n",
      "2022-01-28 19:06:07,642 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:06:07,644 EPOCH 8 done: loss 0.3954 - lr 0.1000000\n",
      "2022-01-28 19:06:15,089 DEV : loss 0.23838350176811218 - f1-score (micro avg)  0.9179\n",
      "2022-01-28 19:06:15,129 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:06:15,131 saving best model\n",
      "2022-01-28 19:06:31,283 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:06:35,056 epoch 9 - iter 14/141 - loss 0.40096695 - samples/sec: 118.84 - lr: 0.100000\n",
      "2022-01-28 19:06:38,396 epoch 9 - iter 28/141 - loss 0.38764918 - samples/sec: 134.17 - lr: 0.100000\n",
      "2022-01-28 19:06:41,913 epoch 9 - iter 42/141 - loss 0.38913933 - samples/sec: 127.50 - lr: 0.100000\n",
      "2022-01-28 19:06:45,156 epoch 9 - iter 56/141 - loss 0.38543028 - samples/sec: 138.17 - lr: 0.100000\n",
      "2022-01-28 19:06:48,235 epoch 9 - iter 70/141 - loss 0.38815418 - samples/sec: 145.56 - lr: 0.100000\n",
      "2022-01-28 19:06:51,604 epoch 9 - iter 84/141 - loss 0.38771437 - samples/sec: 133.08 - lr: 0.100000\n",
      "2022-01-28 19:06:55,420 epoch 9 - iter 98/141 - loss 0.38636371 - samples/sec: 117.44 - lr: 0.100000\n",
      "2022-01-28 19:06:58,742 epoch 9 - iter 112/141 - loss 0.38675000 - samples/sec: 134.97 - lr: 0.100000\n",
      "2022-01-28 19:07:02,511 epoch 9 - iter 126/141 - loss 0.38447224 - samples/sec: 118.93 - lr: 0.100000\n",
      "2022-01-28 19:07:05,635 epoch 9 - iter 140/141 - loss 0.38537671 - samples/sec: 143.42 - lr: 0.100000\n",
      "2022-01-28 19:07:05,759 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:07:05,760 EPOCH 9 done: loss 0.3854 - lr 0.1000000\n",
      "2022-01-28 19:07:14,045 DEV : loss 0.2513364851474762 - f1-score (micro avg)  0.9147\n",
      "2022-01-28 19:07:14,085 BAD EPOCHS (no improvement): 1\n",
      "2022-01-28 19:07:14,087 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:07:17,288 epoch 10 - iter 14/141 - loss 0.37995592 - samples/sec: 140.07 - lr: 0.100000\n",
      "2022-01-28 19:07:20,691 epoch 10 - iter 28/141 - loss 0.38702187 - samples/sec: 131.69 - lr: 0.100000\n",
      "2022-01-28 19:07:24,239 epoch 10 - iter 42/141 - loss 0.38789792 - samples/sec: 126.36 - lr: 0.100000\n",
      "2022-01-28 19:07:27,753 epoch 10 - iter 56/141 - loss 0.38430631 - samples/sec: 127.61 - lr: 0.100000\n",
      "2022-01-28 19:07:31,075 epoch 10 - iter 70/141 - loss 0.38453417 - samples/sec: 134.93 - lr: 0.100000\n",
      "2022-01-28 19:07:34,419 epoch 10 - iter 84/141 - loss 0.37880834 - samples/sec: 134.05 - lr: 0.100000\n",
      "2022-01-28 19:07:37,785 epoch 10 - iter 98/141 - loss 0.37724967 - samples/sec: 133.18 - lr: 0.100000\n",
      "2022-01-28 19:07:41,290 epoch 10 - iter 112/141 - loss 0.37819526 - samples/sec: 127.87 - lr: 0.100000\n",
      "2022-01-28 19:07:44,592 epoch 10 - iter 126/141 - loss 0.37770307 - samples/sec: 135.71 - lr: 0.100000\n",
      "2022-01-28 19:07:47,997 epoch 10 - iter 140/141 - loss 0.37873632 - samples/sec: 131.81 - lr: 0.100000\n",
      "2022-01-28 19:07:48,070 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:07:48,071 EPOCH 10 done: loss 0.3788 - lr 0.1000000\n",
      "2022-01-28 19:07:55,445 DEV : loss 0.23448392748832703 - f1-score (micro avg)  0.9206\n",
      "2022-01-28 19:07:55,487 BAD EPOCHS (no improvement): 0\n",
      "2022-01-28 19:07:55,489 saving best model\n",
      "2022-01-28 19:08:27,984 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-28 19:08:27,986 loading file resources\\taggers\\example-upos\\best-model.pt\n",
      "2022-01-28 19:08:39,846 0.9251\t0.9251\t0.9251\t0.9251\n",
      "2022-01-28 19:08:39,847 \n",
      "Results:\n",
      "- F-score (micro) 0.9251\n",
      "- F-score (macro) 0.8646\n",
      "- Accuracy 0.9251\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN     0.8748    0.9152    0.8945      2511\n",
      "       PROPN     0.9271    0.8996    0.9131      2162\n",
      "       PUNCT     0.9982    1.0000    0.9991      1623\n",
      "        VERB     0.9454    0.9342    0.9398      1261\n",
      "         ADP     0.9334    0.9560    0.9446      1114\n",
      "        PRON     0.9321    0.9798    0.9553       644\n",
      "         ADJ     0.8614    0.7131    0.7803       488\n",
      "         NUM     0.9352    0.9766    0.9554       384\n",
      "       CCONJ     0.9783    0.9945    0.9863       362\n",
      "         ADV     0.8433    0.7775    0.8090       346\n",
      "         DET     0.9599    0.9120    0.9353       341\n",
      "         AUX     0.9306    0.9956    0.9620       229\n",
      "       SCONJ     0.8500    0.7887    0.8182       194\n",
      "        PART     0.9149    0.9663    0.9399        89\n",
      "         SYM     1.0000    1.0000    1.0000         6\n",
      "           X     0.0000    0.0000    0.0000         2\n",
      "\n",
      "   micro avg     0.9251    0.9251    0.9251     11756\n",
      "   macro avg     0.8678    0.8631    0.8646     11756\n",
      "weighted avg     0.9247    0.9251    0.9243     11756\n",
      " samples avg     0.9251    0.9251    0.9251     11756\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 19:08:39,848 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9251446070091868,\n",
       " 'dev_score_history': [0.8068872916831213,\n",
       "  0.8681778690466788,\n",
       "  0.8908459047468604,\n",
       "  0.9098017534160019,\n",
       "  0.912487165310797,\n",
       "  0.9161203696390492,\n",
       "  0.91691019666693,\n",
       "  0.9179369718031751,\n",
       "  0.9146986809888634,\n",
       "  0.9206223836979701],\n",
       " 'train_loss_history': [1.4472624681742532,\n",
       "  0.6738529138880467,\n",
       "  0.5256503110518002,\n",
       "  0.4710184044683768,\n",
       "  0.44136488294509596,\n",
       "  0.41903373262080146,\n",
       "  0.4103610988210632,\n",
       "  0.395417884932029,\n",
       "  0.3853790417962727,\n",
       "  0.3788037536759918],\n",
       " 'dev_loss_history': [tensor(0.6212, device='cuda:0'),\n",
       "  tensor(0.3889, device='cuda:0'),\n",
       "  tensor(0.3303, device='cuda:0'),\n",
       "  tensor(0.2770, device='cuda:0'),\n",
       "  tensor(0.2565, device='cuda:0'),\n",
       "  tensor(0.2481, device='cuda:0'),\n",
       "  tensor(0.2527, device='cuda:0'),\n",
       "  tensor(0.2384, device='cuda:0'),\n",
       "  tensor(0.2513, device='cuda:0'),\n",
       "  tensor(0.2345, device='cuda:0')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "corpus = UD_INDONESIAN()\n",
    "print(corpus)\n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'upos'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('id'),\n",
    "    WordEmbeddings('id-crawl')\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-upos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
