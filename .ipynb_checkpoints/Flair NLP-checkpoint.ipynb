{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "en9bUDg3P6Pm",
    "outputId": "828d7fe2-6674-4c2f-ac81-3cd3582657a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (4.61.2)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "Collecting gdown==3.12.2\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: regex in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (2020.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: lxml in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (4.5.2)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (3.4.2)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (0.23.2)\n",
      "Collecting transformers>=4.0.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Requirement already satisfied: gensim>=3.4.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from flair) (3.4.0)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting more-itertools~=8.8.0\n",
      "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.26.0->flair) (0.4.1)\n",
      "Requirement already satisfied: six in d:\\coding\\software\\anaconda\\lib\\site-packages (from langdetect->flair) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in d:\\coding\\software\\anaconda\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: requests[socks] in d:\\coding\\software\\anaconda\\lib\\site-packages (from gdown==3.12.2->flair) (2.24.0)\n",
      "Requirement already satisfied: filelock in d:\\coding\\software\\anaconda\\lib\\site-packages (from gdown==3.12.2->flair) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in d:\\coding\\software\\anaconda\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in d:\\coding\\software\\anaconda\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (0.6)\n",
      "Requirement already satisfied: numpy in d:\\coding\\software\\anaconda\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.19.5)\n",
      "Requirement already satisfied: future in d:\\coding\\software\\anaconda\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (0.18.2)\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\coding\\software\\anaconda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from matplotlib>=2.2.3->flair) (8.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\coding\\software\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from transformers>=4.0.0->flair) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from transformers>=4.0.0->flair) (5.3.1)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from gensim>=3.4.0->flair) (2.2.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in d:\\coding\\software\\anaconda\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\coding\\software\\anaconda\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding\\software\\anaconda\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\coding\\software\\anaconda\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (1.25.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in d:\\coding\\software\\anaconda\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\coding\\software\\anaconda\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.3.0)\n",
      "Requirement already satisfied: click in d:\\coding\\software\\anaconda\\lib\\site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
      "Requirement already satisfied: boto3 in d:\\coding\\software\\anaconda\\lib\\site-packages (from smart_open>=1.2.1->gensim>=3.4.0->flair) (1.15.12)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in d:\\coding\\software\\anaconda\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.12 in d:\\coding\\software\\anaconda\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim>=3.4.0->flair) (1.18.12)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in d:\\coding\\software\\anaconda\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
      "Building wheels for collected packages: langdetect, ftfy, gdown, mpld3, sqlitedict, wikipedia-api, overrides\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=4f8986a1cb24436cf91674a123d8b2c4c1dc682d54f383d7ccbb1b6284cb0572\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\c5\\96\\8a\\f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for ftfy (setup.py): started\n",
      "  Building wheel for ftfy (setup.py): finished with status 'done'\n",
      "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41920 sha256=6c29bdfed97caa95e41756876cc300e246f1fea9950867b0f9cecfacfe1f2236\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\19\\f5\\38\\273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=96dfd1dfb783ba061e655fe64b9e3859a184dca881c6f85761e14f84b42b5d4e\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\ba\\e0\\7e\\726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "nxviz 0.6.2 requires hypothesis==4.34.0, which is not installed.\n",
      "nxviz 0.6.2 requires sphinxcontrib-fulltoc==1.2.0, which is not installed.\n",
      "nxviz 0.6.2 requires cryptography==2.7, but you'll have cryptography 3.1.1 which is incompatible.\n",
      "nxviz 0.6.2 requires matplotlib==3.1.1, but you'll have matplotlib 3.4.2 which is incompatible.\n",
      "nxviz 0.6.2 requires more-itertools==7.2.0, but you'll have more-itertools 8.8.0 which is incompatible.\n",
      "nxviz 0.6.2 requires networkx==2.3, but you'll have networkx 2.5 which is incompatible.\n",
      "nxviz 0.6.2 requires numpy==1.17.1, but you'll have numpy 1.19.5 which is incompatible.\n",
      "nxviz 0.6.2 requires palettable==3.1.1, but you'll have palettable 3.3.0 which is incompatible.\n",
      "nxviz 0.6.2 requires pandas==0.25.1, but you'll have pandas 1.1.2 which is incompatible.\n",
      "nxviz 0.6.2 requires pytest==5.1.2, but you'll have pytest 0.0.0 which is incompatible.\n",
      "nxviz 0.6.2 requires PyYAML==5.1.2, but you'll have pyyaml 5.3.1 which is incompatible.\n",
      "nxviz 0.6.2 requires seaborn==0.9.0, but you'll have seaborn 0.11.0 which is incompatible.\n",
      "nxviz 0.6.2 requires setuptools==41.2.0, but you'll have setuptools 50.3.0.post20201006 which is incompatible.\n",
      "konoha 4.6.5 requires requests<3.0.0,>=2.25.1, but you'll have requests 2.24.0 which is incompatible.\n",
      "huggingface-hub 0.4.0 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116683 sha256=a076f09279e4d9d232acafb79569f71710655d600864ddd37b414b07cf24d1f1\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\26\\70\\6a\\1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14381 sha256=32bf0dd88fbbd8def7cdf89a59253b3ff4c3b5a2a9929be0a00b2673616655c8\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\af\\94\\06\\18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
      "  Building wheel for wikipedia-api (setup.py): started\n",
      "  Building wheel for wikipedia-api (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13467 sha256=b885e3fe1a8d0bd1ed633d3877c543a3f4d09a288516962588e2138ba9767a92\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\d3\\24\\56\\58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10179 sha256=8d7d24b54ea7d500c1a00e1983ac0cd3fc6291aceedece3612181383f86ea2ec\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\3a\\0d\\38\\01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
      "Successfully built langdetect ftfy gdown mpld3 sqlitedict wikipedia-api overrides\n",
      "Installing collected packages: tabulate, langdetect, ftfy, gdown, importlib-metadata, overrides, konoha, mpld3, sentencepiece, bpemb, sacremoses, tokenizers, huggingface-hub, transformers, deprecated, sqlitedict, more-itertools, wikipedia-api, janome, segtok, conllu, flair\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 1.7.0\n",
      "    Uninstalling importlib-metadata-1.7.0:\n",
      "      Successfully uninstalled importlib-metadata-1.7.0\n",
      "  Attempting uninstall: more-itertools\n",
      "    Found existing installation: more-itertools 8.5.0\n",
      "    Uninstalling more-itertools-8.5.0:\n",
      "      Successfully uninstalled more-itertools-8.5.0\n",
      "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-1.7.0 tabulate-0.8.9 tokenizers-0.10.3 transformers-4.15.0 wikipedia-api-0.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Byjn8iEai1gu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-N8O7y3ezxH2",
    "outputId": "07fcee7a-31f1-4cea-f29a-cbb8ef90211b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:48:51,076 Reading data from C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\n",
      "2022-01-20 15:48:51,077 Train: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-train.conllu\n",
      "2022-01-20 15:48:51,078 Dev: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-dev.conllu\n",
      "2022-01-20 15:48:51,079 Test: C:\\Users\\ASUS\\.flair\\datasets\\ud_indonesian\\id_gsd-ud-test.conllu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated function (or staticmethod) load_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \n",
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated method make_tag_dictionary. (Use 'make_label_dictionary' instead.) -- Deprecated since version 0.8.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'O', b'PROPN', b'AUX', b'DET', b'NOUN', b'PRON', b'VERB', b'ADP', b'PUNCT', b'ADV', b'CCONJ', b'SCONJ', b'NUM', b'ADJ', b'PART', b'SYM', b'INTJ', b'X', b'<START>', b'<STOP>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Software\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:49:04,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,239 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'id-crawl'\n",
      "      (embedding): Embedding(1000000, 300)\n",
      "    )\n",
      "    (list_embedding_1): WordEmbeddings(\n",
      "      'id'\n",
      "      (embedding): Embedding(300686, 300)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (rnn): LSTM(600, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-01-20 15:49:04,240 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,241 Corpus: \"Corpus: 4482 train + 559 dev + 557 test sentences\"\n",
      "2022-01-20 15:49:04,242 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,242 Parameters:\n",
      "2022-01-20 15:49:04,243  - learning_rate: \"0.1\"\n",
      "2022-01-20 15:49:04,245  - mini_batch_size: \"32\"\n",
      "2022-01-20 15:49:04,246  - patience: \"3\"\n",
      "2022-01-20 15:49:04,248  - anneal_factor: \"0.5\"\n",
      "2022-01-20 15:49:04,249  - max_epochs: \"10\"\n",
      "2022-01-20 15:49:04,252  - shuffle: \"True\"\n",
      "2022-01-20 15:49:04,254  - train_with_dev: \"False\"\n",
      "2022-01-20 15:49:04,255  - batch_growth_annealing: \"False\"\n",
      "2022-01-20 15:49:04,257 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,259 Model training base path: \"resources\\taggers\\example-universal-pos\"\n",
      "2022-01-20 15:49:04,260 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,261 Device: cuda:0\n",
      "2022-01-20 15:49:04,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:04,263 Embeddings storage mode: gpu\n",
      "2022-01-20 15:49:04,266 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:08,341 epoch 1 - iter 14/141 - loss 2.82165917 - samples/sec: 110.00 - lr: 0.100000\n",
      "2022-01-20 15:49:12,225 epoch 1 - iter 28/141 - loss 2.46792793 - samples/sec: 115.43 - lr: 0.100000\n",
      "2022-01-20 15:49:15,520 epoch 1 - iter 42/141 - loss 2.21808006 - samples/sec: 135.97 - lr: 0.100000\n",
      "2022-01-20 15:49:19,064 epoch 1 - iter 56/141 - loss 2.03649914 - samples/sec: 126.48 - lr: 0.100000\n",
      "2022-01-20 15:49:23,131 epoch 1 - iter 70/141 - loss 1.88714541 - samples/sec: 110.22 - lr: 0.100000\n",
      "2022-01-20 15:49:27,634 epoch 1 - iter 84/141 - loss 1.76754314 - samples/sec: 99.49 - lr: 0.100000\n",
      "2022-01-20 15:49:32,084 epoch 1 - iter 98/141 - loss 1.66223759 - samples/sec: 100.70 - lr: 0.100000\n",
      "2022-01-20 15:49:36,180 epoch 1 - iter 112/141 - loss 1.57620549 - samples/sec: 109.45 - lr: 0.100000\n",
      "2022-01-20 15:49:40,341 epoch 1 - iter 126/141 - loss 1.49847917 - samples/sec: 107.76 - lr: 0.100000\n",
      "2022-01-20 15:49:43,832 epoch 1 - iter 140/141 - loss 1.43472244 - samples/sec: 128.40 - lr: 0.100000\n",
      "2022-01-20 15:49:43,874 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:49:43,875 EPOCH 1 done: loss 1.4345 - lr 0.1000000\n",
      "2022-01-20 15:49:52,221 DEV : loss 0.6019213795661926 - f1-score (micro avg)  0.8173\n",
      "2022-01-20 15:49:52,229 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:49:52,230 saving best model\n",
      "2022-01-20 15:50:11,368 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:50:14,895 epoch 2 - iter 14/141 - loss 0.79504646 - samples/sec: 127.21 - lr: 0.100000\n",
      "2022-01-20 15:50:18,071 epoch 2 - iter 28/141 - loss 0.77103652 - samples/sec: 141.16 - lr: 0.100000\n",
      "2022-01-20 15:50:21,125 epoch 2 - iter 42/141 - loss 0.75001673 - samples/sec: 146.79 - lr: 0.100000\n",
      "2022-01-20 15:50:24,773 epoch 2 - iter 56/141 - loss 0.72962919 - samples/sec: 122.84 - lr: 0.100000\n",
      "2022-01-20 15:50:28,020 epoch 2 - iter 70/141 - loss 0.71364567 - samples/sec: 138.06 - lr: 0.100000\n",
      "2022-01-20 15:50:31,410 epoch 2 - iter 84/141 - loss 0.69960640 - samples/sec: 132.21 - lr: 0.100000\n",
      "2022-01-20 15:50:34,639 epoch 2 - iter 98/141 - loss 0.68394728 - samples/sec: 138.84 - lr: 0.100000\n",
      "2022-01-20 15:50:37,415 epoch 2 - iter 112/141 - loss 0.67335816 - samples/sec: 161.47 - lr: 0.100000\n",
      "2022-01-20 15:50:40,795 epoch 2 - iter 126/141 - loss 0.66185933 - samples/sec: 132.70 - lr: 0.100000\n",
      "2022-01-20 15:50:44,194 epoch 2 - iter 140/141 - loss 0.65405899 - samples/sec: 131.83 - lr: 0.100000\n",
      "2022-01-20 15:50:44,363 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:50:44,364 EPOCH 2 done: loss 0.6540 - lr 0.1000000\n",
      "2022-01-20 15:50:51,984 DEV : loss 0.366698294878006 - f1-score (micro avg)  0.8807\n",
      "2022-01-20 15:50:51,991 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:50:51,993 saving best model\n",
      "2022-01-20 15:51:08,086 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:51:11,129 epoch 3 - iter 14/141 - loss 0.56332486 - samples/sec: 147.47 - lr: 0.100000\n",
      "2022-01-20 15:51:14,586 epoch 3 - iter 28/141 - loss 0.55630014 - samples/sec: 129.70 - lr: 0.100000\n",
      "2022-01-20 15:51:18,204 epoch 3 - iter 42/141 - loss 0.54340371 - samples/sec: 123.86 - lr: 0.100000\n",
      "2022-01-20 15:51:21,425 epoch 3 - iter 56/141 - loss 0.53554393 - samples/sec: 139.17 - lr: 0.100000\n",
      "2022-01-20 15:51:24,907 epoch 3 - iter 70/141 - loss 0.53624927 - samples/sec: 128.72 - lr: 0.100000\n",
      "2022-01-20 15:51:27,859 epoch 3 - iter 84/141 - loss 0.53072885 - samples/sec: 151.87 - lr: 0.100000\n",
      "2022-01-20 15:51:30,776 epoch 3 - iter 98/141 - loss 0.53013566 - samples/sec: 153.62 - lr: 0.100000\n",
      "2022-01-20 15:51:33,949 epoch 3 - iter 112/141 - loss 0.52776558 - samples/sec: 141.28 - lr: 0.100000\n",
      "2022-01-20 15:51:37,402 epoch 3 - iter 126/141 - loss 0.52486195 - samples/sec: 129.78 - lr: 0.100000\n",
      "2022-01-20 15:51:40,842 epoch 3 - iter 140/141 - loss 0.52082324 - samples/sec: 130.26 - lr: 0.100000\n",
      "2022-01-20 15:51:40,898 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:51:40,900 EPOCH 3 done: loss 0.5207 - lr 0.1000000\n",
      "2022-01-20 15:51:48,251 DEV : loss 0.30400601029396057 - f1-score (micro avg)  0.898\n",
      "2022-01-20 15:51:48,258 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:51:48,259 saving best model\n",
      "2022-01-20 15:52:04,573 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:52:08,148 epoch 4 - iter 14/141 - loss 0.47729808 - samples/sec: 125.55 - lr: 0.100000\n",
      "2022-01-20 15:52:11,764 epoch 4 - iter 28/141 - loss 0.47245694 - samples/sec: 123.91 - lr: 0.100000\n",
      "2022-01-20 15:52:15,163 epoch 4 - iter 42/141 - loss 0.47345434 - samples/sec: 131.86 - lr: 0.100000\n",
      "2022-01-20 15:52:18,051 epoch 4 - iter 56/141 - loss 0.47374844 - samples/sec: 155.22 - lr: 0.100000\n",
      "2022-01-20 15:52:21,235 epoch 4 - iter 70/141 - loss 0.47794065 - samples/sec: 140.74 - lr: 0.100000\n",
      "2022-01-20 15:52:24,616 epoch 4 - iter 84/141 - loss 0.47498202 - samples/sec: 132.57 - lr: 0.100000\n",
      "2022-01-20 15:52:28,382 epoch 4 - iter 98/141 - loss 0.47676317 - samples/sec: 119.08 - lr: 0.100000\n",
      "2022-01-20 15:52:31,653 epoch 4 - iter 112/141 - loss 0.47623257 - samples/sec: 136.99 - lr: 0.100000\n",
      "2022-01-20 15:52:34,944 epoch 4 - iter 126/141 - loss 0.47438004 - samples/sec: 136.22 - lr: 0.100000\n",
      "2022-01-20 15:52:37,928 epoch 4 - iter 140/141 - loss 0.47200585 - samples/sec: 150.20 - lr: 0.100000\n",
      "2022-01-20 15:52:38,199 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:52:38,200 EPOCH 4 done: loss 0.4718 - lr 0.1000000\n",
      "2022-01-20 15:52:46,335 DEV : loss 0.32718971371650696 - f1-score (micro avg)  0.8867\n",
      "2022-01-20 15:52:46,340 BAD EPOCHS (no improvement): 1\n",
      "2022-01-20 15:52:46,343 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:52:49,634 epoch 5 - iter 14/141 - loss 0.43740832 - samples/sec: 136.26 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:52:52,723 epoch 5 - iter 28/141 - loss 0.44033745 - samples/sec: 145.09 - lr: 0.100000\n",
      "2022-01-20 15:52:55,823 epoch 5 - iter 42/141 - loss 0.44294033 - samples/sec: 144.62 - lr: 0.100000\n",
      "2022-01-20 15:52:59,099 epoch 5 - iter 56/141 - loss 0.44799397 - samples/sec: 136.83 - lr: 0.100000\n",
      "2022-01-20 15:53:02,581 epoch 5 - iter 70/141 - loss 0.44528486 - samples/sec: 128.71 - lr: 0.100000\n",
      "2022-01-20 15:53:05,799 epoch 5 - iter 84/141 - loss 0.44377835 - samples/sec: 139.27 - lr: 0.100000\n",
      "2022-01-20 15:53:09,202 epoch 5 - iter 98/141 - loss 0.44458514 - samples/sec: 131.71 - lr: 0.100000\n",
      "2022-01-20 15:53:12,404 epoch 5 - iter 112/141 - loss 0.44571928 - samples/sec: 140.04 - lr: 0.100000\n",
      "2022-01-20 15:53:15,845 epoch 5 - iter 126/141 - loss 0.44534921 - samples/sec: 130.23 - lr: 0.100000\n",
      "2022-01-20 15:53:18,942 epoch 5 - iter 140/141 - loss 0.44520944 - samples/sec: 144.74 - lr: 0.100000\n",
      "2022-01-20 15:53:18,984 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:53:18,985 EPOCH 5 done: loss 0.4452 - lr 0.1000000\n",
      "2022-01-20 15:53:26,588 DEV : loss 0.27849557995796204 - f1-score (micro avg)  0.905\n",
      "2022-01-20 15:53:26,595 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:53:26,597 saving best model\n",
      "2022-01-20 15:53:42,634 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:53:46,077 epoch 6 - iter 14/141 - loss 0.43485369 - samples/sec: 130.29 - lr: 0.100000\n",
      "2022-01-20 15:53:49,583 epoch 6 - iter 28/141 - loss 0.42993379 - samples/sec: 127.83 - lr: 0.100000\n",
      "2022-01-20 15:53:53,206 epoch 6 - iter 42/141 - loss 0.42569244 - samples/sec: 123.79 - lr: 0.100000\n",
      "2022-01-20 15:53:56,193 epoch 6 - iter 56/141 - loss 0.42206160 - samples/sec: 150.17 - lr: 0.100000\n",
      "2022-01-20 15:53:59,158 epoch 6 - iter 70/141 - loss 0.42092200 - samples/sec: 151.24 - lr: 0.100000\n",
      "2022-01-20 15:54:01,985 epoch 6 - iter 84/141 - loss 0.42035161 - samples/sec: 158.54 - lr: 0.100000\n",
      "2022-01-20 15:54:05,413 epoch 6 - iter 98/141 - loss 0.42190327 - samples/sec: 130.78 - lr: 0.100000\n",
      "2022-01-20 15:54:08,608 epoch 6 - iter 112/141 - loss 0.42220531 - samples/sec: 140.35 - lr: 0.100000\n",
      "2022-01-20 15:54:11,984 epoch 6 - iter 126/141 - loss 0.42517987 - samples/sec: 132.76 - lr: 0.100000\n",
      "2022-01-20 15:54:15,155 epoch 6 - iter 140/141 - loss 0.42432643 - samples/sec: 141.37 - lr: 0.100000\n",
      "2022-01-20 15:54:15,219 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:54:15,220 EPOCH 6 done: loss 0.4243 - lr 0.1000000\n",
      "2022-01-20 15:54:23,377 DEV : loss 0.2557109296321869 - f1-score (micro avg)  0.9149\n",
      "2022-01-20 15:54:23,385 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:54:23,387 saving best model\n",
      "2022-01-20 15:54:39,715 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:54:42,786 epoch 7 - iter 14/141 - loss 0.39637665 - samples/sec: 146.12 - lr: 0.100000\n",
      "2022-01-20 15:54:46,011 epoch 7 - iter 28/141 - loss 0.41571223 - samples/sec: 138.93 - lr: 0.100000\n",
      "2022-01-20 15:54:49,252 epoch 7 - iter 42/141 - loss 0.41075291 - samples/sec: 138.35 - lr: 0.100000\n",
      "2022-01-20 15:54:52,294 epoch 7 - iter 56/141 - loss 0.41386056 - samples/sec: 147.41 - lr: 0.100000\n",
      "2022-01-20 15:54:55,768 epoch 7 - iter 70/141 - loss 0.41345917 - samples/sec: 129.01 - lr: 0.100000\n",
      "2022-01-20 15:54:58,981 epoch 7 - iter 84/141 - loss 0.41369943 - samples/sec: 139.46 - lr: 0.100000\n",
      "2022-01-20 15:55:02,355 epoch 7 - iter 98/141 - loss 0.41417004 - samples/sec: 132.85 - lr: 0.100000\n",
      "2022-01-20 15:55:05,823 epoch 7 - iter 112/141 - loss 0.41349246 - samples/sec: 129.23 - lr: 0.100000\n",
      "2022-01-20 15:55:09,056 epoch 7 - iter 126/141 - loss 0.41226065 - samples/sec: 138.70 - lr: 0.100000\n",
      "2022-01-20 15:55:12,346 epoch 7 - iter 140/141 - loss 0.41095982 - samples/sec: 136.19 - lr: 0.100000\n",
      "2022-01-20 15:55:12,481 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:55:12,482 EPOCH 7 done: loss 0.4110 - lr 0.1000000\n",
      "2022-01-20 15:55:19,714 DEV : loss 0.25018325448036194 - f1-score (micro avg)  0.9162\n",
      "2022-01-20 15:55:19,718 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:55:19,722 saving best model\n",
      "2022-01-20 15:55:36,018 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:55:39,188 epoch 8 - iter 14/141 - loss 0.40891808 - samples/sec: 141.57 - lr: 0.100000\n",
      "2022-01-20 15:55:42,374 epoch 8 - iter 28/141 - loss 0.39958121 - samples/sec: 140.71 - lr: 0.100000\n",
      "2022-01-20 15:55:45,423 epoch 8 - iter 42/141 - loss 0.40034811 - samples/sec: 146.95 - lr: 0.100000\n",
      "2022-01-20 15:55:49,305 epoch 8 - iter 56/141 - loss 0.39772563 - samples/sec: 115.47 - lr: 0.100000\n",
      "2022-01-20 15:55:52,201 epoch 8 - iter 70/141 - loss 0.39918361 - samples/sec: 154.73 - lr: 0.100000\n",
      "2022-01-20 15:55:55,512 epoch 8 - iter 84/141 - loss 0.39950869 - samples/sec: 135.42 - lr: 0.100000\n",
      "2022-01-20 15:55:58,619 epoch 8 - iter 98/141 - loss 0.39820852 - samples/sec: 144.27 - lr: 0.100000\n",
      "2022-01-20 15:56:01,782 epoch 8 - iter 112/141 - loss 0.39679906 - samples/sec: 141.68 - lr: 0.100000\n",
      "2022-01-20 15:56:05,033 epoch 8 - iter 126/141 - loss 0.39785872 - samples/sec: 137.92 - lr: 0.100000\n",
      "2022-01-20 15:56:08,435 epoch 8 - iter 140/141 - loss 0.39792258 - samples/sec: 131.79 - lr: 0.100000\n",
      "2022-01-20 15:56:08,529 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:56:08,531 EPOCH 8 done: loss 0.3980 - lr 0.1000000\n",
      "2022-01-20 15:56:16,155 DEV : loss 0.24203816056251526 - f1-score (micro avg)  0.9205\n",
      "2022-01-20 15:56:16,161 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:56:16,162 saving best model\n",
      "2022-01-20 15:56:32,205 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:56:35,796 epoch 9 - iter 14/141 - loss 0.37716012 - samples/sec: 124.89 - lr: 0.100000\n",
      "2022-01-20 15:56:38,928 epoch 9 - iter 28/141 - loss 0.38267083 - samples/sec: 143.11 - lr: 0.100000\n",
      "2022-01-20 15:56:42,206 epoch 9 - iter 42/141 - loss 0.38877322 - samples/sec: 136.74 - lr: 0.100000\n",
      "2022-01-20 15:56:45,515 epoch 9 - iter 56/141 - loss 0.39175084 - samples/sec: 135.47 - lr: 0.100000\n",
      "2022-01-20 15:56:48,924 epoch 9 - iter 70/141 - loss 0.39165669 - samples/sec: 131.49 - lr: 0.100000\n",
      "2022-01-20 15:56:52,217 epoch 9 - iter 84/141 - loss 0.39296018 - samples/sec: 136.10 - lr: 0.100000\n",
      "2022-01-20 15:56:55,786 epoch 9 - iter 98/141 - loss 0.39277973 - samples/sec: 125.55 - lr: 0.100000\n",
      "2022-01-20 15:56:58,926 epoch 9 - iter 112/141 - loss 0.39268183 - samples/sec: 142.73 - lr: 0.100000\n",
      "2022-01-20 15:57:02,167 epoch 9 - iter 126/141 - loss 0.38949749 - samples/sec: 138.34 - lr: 0.100000\n",
      "2022-01-20 15:57:05,338 epoch 9 - iter 140/141 - loss 0.38573284 - samples/sec: 141.36 - lr: 0.100000\n",
      "2022-01-20 15:57:05,402 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:57:05,404 EPOCH 9 done: loss 0.3857 - lr 0.1000000\n",
      "2022-01-20 15:57:15,135 DEV : loss 0.23911811411380768 - f1-score (micro avg)  0.9226\n",
      "2022-01-20 15:57:15,143 BAD EPOCHS (no improvement): 0\n",
      "2022-01-20 15:57:15,161 saving best model\n",
      "2022-01-20 15:57:31,718 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:57:34,991 epoch 10 - iter 14/141 - loss 0.38258890 - samples/sec: 137.13 - lr: 0.100000\n",
      "2022-01-20 15:57:38,121 epoch 10 - iter 28/141 - loss 0.38398588 - samples/sec: 143.23 - lr: 0.100000\n",
      "2022-01-20 15:57:41,617 epoch 10 - iter 42/141 - loss 0.38679738 - samples/sec: 128.20 - lr: 0.100000\n",
      "2022-01-20 15:57:45,495 epoch 10 - iter 56/141 - loss 0.38834959 - samples/sec: 115.60 - lr: 0.100000\n",
      "2022-01-20 15:57:48,924 epoch 10 - iter 70/141 - loss 0.38498271 - samples/sec: 130.66 - lr: 0.100000\n",
      "2022-01-20 15:57:52,443 epoch 10 - iter 84/141 - loss 0.38409164 - samples/sec: 127.40 - lr: 0.100000\n",
      "2022-01-20 15:57:55,384 epoch 10 - iter 98/141 - loss 0.38293347 - samples/sec: 152.39 - lr: 0.100000\n",
      "2022-01-20 15:57:58,347 epoch 10 - iter 112/141 - loss 0.38175534 - samples/sec: 151.30 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:58:01,714 epoch 10 - iter 126/141 - loss 0.38154903 - samples/sec: 133.10 - lr: 0.100000\n",
      "2022-01-20 15:58:05,203 epoch 10 - iter 140/141 - loss 0.38134922 - samples/sec: 128.48 - lr: 0.100000\n",
      "2022-01-20 15:58:05,454 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:58:05,455 EPOCH 10 done: loss 0.3812 - lr 0.1000000\n",
      "2022-01-20 15:58:13,455 DEV : loss 0.23544906079769135 - f1-score (micro avg)  0.9206\n",
      "2022-01-20 15:58:13,459 BAD EPOCHS (no improvement): 1\n",
      "2022-01-20 15:58:30,222 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-20 15:58:30,227 loading file resources\\taggers\\example-universal-pos\\best-model.pt\n",
      "2022-01-20 15:58:45,843 0.9285\t0.9285\t0.9285\t0.9285\n",
      "2022-01-20 15:58:45,845 \n",
      "Results:\n",
      "- F-score (micro) 0.9285\n",
      "- F-score (macro) 0.8612\n",
      "- Accuracy 0.9285\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NOUN     0.9251    0.8702    0.8968      2511\n",
      "       PROPN     0.8922    0.9570    0.9235      2162\n",
      "       PUNCT     0.9969    0.9994    0.9982      1623\n",
      "        VERB     0.9446    0.9334    0.9390      1261\n",
      "         ADP     0.9429    0.9479    0.9454      1114\n",
      "        PRON     0.9503    0.9798    0.9648       644\n",
      "         ADJ     0.8080    0.7848    0.7963       488\n",
      "         NUM     0.9764    0.9688    0.9725       384\n",
      "       CCONJ     0.9807    0.9807    0.9807       362\n",
      "         ADV     0.8284    0.8092    0.8187       346\n",
      "         DET     0.9537    0.9062    0.9293       341\n",
      "         AUX     0.9421    0.9956    0.9682       229\n",
      "       SCONJ     0.8103    0.8144    0.8123       194\n",
      "        PART     0.8866    0.9663    0.9247        89\n",
      "         SYM     1.0000    0.8333    0.9091         6\n",
      "           X     0.0000    0.0000    0.0000         2\n",
      "\n",
      "   micro avg     0.9285    0.9285    0.9285     11756\n",
      "   macro avg     0.8649    0.8592    0.8612     11756\n",
      "weighted avg     0.9286    0.9285    0.9281     11756\n",
      " samples avg     0.9285    0.9285    0.9285     11756\n",
      "\n",
      "2022-01-20 15:58:45,845 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9285471248724055,\n",
       " 'dev_score_history': [0.8173130084511492,\n",
       "  0.8806571360871969,\n",
       "  0.8979543479977885,\n",
       "  0.8867388042018798,\n",
       "  0.9049838085459284,\n",
       "  0.9148566463944396,\n",
       "  0.9161993523418372,\n",
       "  0.920464418292394,\n",
       "  0.9225969512676724,\n",
       "  0.9206223836979701],\n",
       " 'train_loss_history': [1.434530851249934,\n",
       "  0.654039959571219,\n",
       "  0.520740862894408,\n",
       "  0.47183696242962836,\n",
       "  0.44519814952104475,\n",
       "  0.4243022352304377,\n",
       "  0.41101335890004587,\n",
       "  0.3979702266306815,\n",
       "  0.3857149224433541,\n",
       "  0.3811704903654151],\n",
       " 'dev_loss_history': [tensor(0.6019, device='cuda:0'),\n",
       "  tensor(0.3667, device='cuda:0'),\n",
       "  tensor(0.3040, device='cuda:0'),\n",
       "  tensor(0.3272, device='cuda:0'),\n",
       "  tensor(0.2785, device='cuda:0'),\n",
       "  tensor(0.2557, device='cuda:0'),\n",
       "  tensor(0.2502, device='cuda:0'),\n",
       "  tensor(0.2420, device='cuda:0'),\n",
       "  tensor(0.2391, device='cuda:0'),\n",
       "  tensor(0.2354, device='cuda:0')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_INDONESIAN)\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'upos'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('id-crawl'),\n",
    "    WordEmbeddings('id'),\n",
    "    #WordEmbeddings('glove'),\n",
    "    #BertEmbeddings('bert-base-multilingual-cased')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-universal-pos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10,\n",
    "              embeddings_storage_mode='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "LqvmmuVf7cPs",
    "outputId": "154b4971-f37b-4973-e420-8395bc18c049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 22:46:21,253 loading file resources/taggers/example-universal-pos/best-model.pt\n",
      "saya <PRON> dan <CCONJ> dia <PRON> kemarin <ADV> pergi <VERB> ke <ADP> pasar <NOUN> bersama <ADP> untuk <SCONJ> membeli <VERB> jeruk <NOUN>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "sentence = Sentence('saya dan dia kemarin pergi ke pasar bersama untuk membeli jeruk')\n",
    "tag_pos = SequenceTagger.load('resources/taggers/example-universal-pos/best-model.pt')\n",
    "tag_pos.predict(sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Flair 2.0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
